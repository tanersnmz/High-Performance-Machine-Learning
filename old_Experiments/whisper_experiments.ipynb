{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f03292-0202-4b3c-8095-de80122f2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24932d8a-0093-47cf-b9ef-7f67537a51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b7d67-c76a-4689-897c-bdb72c8618c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install aihwkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa51112-efb4-498b-a041-e8fbe5f1cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228d97c-ffb6-438c-a8a3-629bc1c79292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import WhisperForConditionalGeneration, GenerationConfig\n",
    "from aihwkit.nn import AnalogLinear\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "\n",
    "# Debug mode flag\n",
    "DEBUG = True\n",
    "\n",
    "def debug_print(*args, **kwargs):\n",
    "    \"\"\"Print debug information only if DEBUG is True\"\"\"\n",
    "    if DEBUG:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "# Define RPU configuration for analog inference\n",
    "rpu_config = InferenceRPUConfig()\n",
    "debug_print(\"Initialized RPU configuration for analog inference\")\n",
    "\n",
    "class AnalogWhisperForConditionalGeneration(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, rpu_config=None, debug=False):\n",
    "        super().__init__(config)\n",
    "        global DEBUG\n",
    "        DEBUG = debug\n",
    "        debug_print(\"Initializing AnalogWhisperForConditionalGeneration model...\")\n",
    "        \n",
    "        # Count the number of linear layers to be replaced\n",
    "        linear_count = sum(1 for _ in self.named_modules() if isinstance(_[1], nn.Linear))\n",
    "        debug_print(f\"Found {linear_count} linear layers to convert to analog\")\n",
    "        \n",
    "        # Replace all linear layers with analog linear layers\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                debug_print(f\"Converting linear layer '{name}' to analog\")\n",
    "                # Create analog layer with same dimensions and bias configuration\n",
    "                analog_layer = AnalogLinear(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    bias=module.bias is not None,\n",
    "                    rpu_config=rpu_config\n",
    "                )\n",
    "                # Navigate to the parent module to replace the layer\n",
    "                parent = self\n",
    "                for n in name.split('.')[:-1]:\n",
    "                    parent = getattr(parent, n)\n",
    "                setattr(parent, name.split('.')[-1], analog_layer)\n",
    "        \n",
    "        debug_print(\"All linear layers converted to analog successfully\")\n",
    "        \n",
    "        # Fix the forced_decoder_ids issue as recommended in GitHub issues\n",
    "        if hasattr(self.generation_config, \"forced_decoder_ids\"):\n",
    "            debug_print(\"Converting forced_decoder_ids to input_ids in model's generation config\")\n",
    "            self.generation_config.input_ids = self.generation_config.forced_decoder_ids\n",
    "            self.generation_config.forced_decoder_ids = None\n",
    "\n",
    "    def transfer_digital_weights(self, digital_model):\n",
    "        \"\"\"\n",
    "        Transfer weights from digital model to analog model using from_digital method\n",
    "        \"\"\"\n",
    "        debug_print(\"Starting weight transfer from digital to analog model...\")\n",
    "        \n",
    "        # Count the number of analog layers to transfer weights to\n",
    "        analog_count = sum(1 for _ in self.named_modules() if isinstance(_[1], AnalogLinear))\n",
    "        debug_print(f\"Found {analog_count} analog layers to transfer weights to\")\n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, AnalogLinear):\n",
    "                debug_print(f\"Transferring weights for analog layer '{name}'\")\n",
    "                # Get the corresponding digital module\n",
    "                digital_module = digital_model\n",
    "                for n in name.split('.'):\n",
    "                    digital_module = getattr(digital_module, n)\n",
    "                # Transfer weights using from_digital method\n",
    "                module.from_digital(digital_module, rpu_config=rpu_config)\n",
    "        \n",
    "        debug_print(\"Weight transfer completed successfully\")\n",
    "\n",
    "# Example: Load pretrained model and create analog inference model\n",
    "debug_print(\"\\nLoading pretrained Whisper model...\")\n",
    "from transformers import WhisperConfig, WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "# Load pretrained model configuration and weights\n",
    "debug_print(\"Loading model configuration...\")\n",
    "config = WhisperConfig.from_pretrained(\"openai/whisper-small.en\")\n",
    "debug_print(\"Loading pretrained weights...\")\n",
    "digital_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small.en\")\n",
    "\n",
    "# Initialize our analog model\n",
    "debug_print(\"\\nCreating analog model...\")\n",
    "analog_model = AnalogWhisperForConditionalGeneration(config, rpu_config=rpu_config, debug=True)\n",
    "\n",
    "# Transfer weights from digital to analog model\n",
    "debug_print(\"\\nTransferring weights from digital to analog model...\")\n",
    "analog_model.transfer_digital_weights(digital_model)\n",
    "\n",
    "# Move model to CUDA\n",
    "debug_print(\"\\nMoving model to CUDA...\")\n",
    "analog_model = analog_model.to('cuda')\n",
    "\n",
    "# Switch to evaluation mode for inference\n",
    "debug_print(\"\\nSwitching to evaluation mode...\")\n",
    "analog_model.eval()\n",
    "\n",
    "# Example input for testing\n",
    "debug_print(\"\\nPreparing test input...\")\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Load processor and a sample audio file\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small.en\")\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "audio_sample = ds[0][\"audio\"]\n",
    "\n",
    "# Process audio into input features\n",
    "input_features = processor(\n",
    "    audio_sample[\"array\"], \n",
    "    sampling_rate=audio_sample[\"sampling_rate\"], \n",
    "    return_tensors=\"pt\"\n",
    ").input_features.to('cuda')\n",
    "debug_print(f\"Input features shape: {input_features.shape}\")\n",
    "\n",
    "# Set up decoder starting token (important!)\n",
    "language = \"en\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# Get language and task token IDs\n",
    "language_token_id = processor.tokenizer.convert_tokens_to_ids(language)\n",
    "task_token_id = processor.tokenizer.convert_tokens_to_ids(task)\n",
    "\n",
    "# Create decoder_input_ids with the right starting tokens\n",
    "decoder_input_ids = torch.tensor([[processor.tokenizer.bos_token_id, language_token_id, task_token_id]]).to('cuda')\n",
    "\n",
    "# Create a custom generation config by copying from model's config\n",
    "import copy\n",
    "\n",
    "# Create a custom generation config by copying from model's config\n",
    "debug_print(\"\\nCreating custom generation config...\")\n",
    "generation_config = copy.deepcopy(analog_model.generation_config)\n",
    "\n",
    "# Ensure forced_decoder_ids is handled properly in custom config\n",
    "if hasattr(generation_config, \"forced_decoder_ids\"):\n",
    "    debug_print(\"Converting forced_decoder_ids to input_ids in custom generation config\")\n",
    "    generation_config.input_ids = generation_config.forced_decoder_ids\n",
    "    generation_config.forced_decoder_ids = None\n",
    "\n",
    "\n",
    "# Ensure forced_decoder_ids is handled properly in custom config\n",
    "if hasattr(generation_config, \"forced_decoder_ids\"):\n",
    "    debug_print(\"Converting forced_decoder_ids to input_ids in custom generation config\")\n",
    "    generation_config.input_ids = generation_config.forced_decoder_ids\n",
    "    generation_config.forced_decoder_ids = None\n",
    "\n",
    "# Run inference\n",
    "debug_print(\"\\nRunning inference...\")\n",
    "with torch.no_grad():\n",
    "    generated_ids = analog_model.generate(\n",
    "        input_features,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    debug_print(f\"Transcription: {transcription}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27023bcc-dd50-4903-9d5b-9bcce737a56a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2e77f-849d-41f9-930d-f23f31ccf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the whisper_analog.py file using standard Python\n",
    "with open('whisper_analog.py', 'w') as f:\n",
    "    f.write('''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers import WhisperForConditionalGeneration, GenerationConfig\n",
    "from aihwkit.nn import AnalogLinear\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "\n",
    "# Debug mode flag\n",
    "DEBUG = True\n",
    "\n",
    "def debug_print(*args, **kwargs):\n",
    "    \"\"\"Print debug information only if DEBUG is True\"\"\"\n",
    "    if DEBUG:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "def verify_weight_transfer(analog_model, digital_model):\n",
    "    \"\"\"Verify that weights have been properly transferred\"\"\"\n",
    "    for name, module in analog_model.named_modules():\n",
    "        if isinstance(module, AnalogLinear):\n",
    "            # Find corresponding digital module\n",
    "            digital_module = digital_model\n",
    "            try:\n",
    "                for n in name.split('.'):\n",
    "                    digital_module = getattr(digital_module, n)\n",
    "                \n",
    "                # Compare weight stats\n",
    "                analog_weight = module.weight.analog_tile.tile.get_weights()[0]\n",
    "                digital_weight = digital_module.weight.data\n",
    "                \n",
    "                print(f\"Layer: {name}\")\n",
    "                print(f\"  Digital weight stats: min={digital_weight.min():.4f}, max={digital_weight.max():.4f}, mean={digital_weight.mean():.4f}\")\n",
    "                print(f\"  Analog weight stats: min={analog_weight.min():.4f}, max={analog_weight.max():.4f}, mean={analog_weight.mean():.4f}\")\n",
    "                \n",
    "                # Check for large differences\n",
    "                if abs(digital_weight.mean() - analog_weight.mean()) > 0.1:\n",
    "                    print(\"  WARNING: Large difference in mean values!\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error comparing layer {name}: {e}\")\n",
    "                \n",
    "    print(\"Weight verification complete\")\n",
    "\n",
    "class AnalogWhisperForConditionalGeneration(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, rpu_config=None, debug=False):\n",
    "        super().__init__(config)\n",
    "        global DEBUG\n",
    "        DEBUG = debug\n",
    "        # Store rpu_config as an instance attribute - THIS IS THE KEY FIX\n",
    "        self.rpu_config = rpu_config\n",
    "        debug_print(\"Initializing AnalogWhisperForConditionalGeneration model...\")\n",
    "        \n",
    "        # Count the number of linear layers to be replaced\n",
    "        linear_count = sum(1 for _ in self.named_modules() if isinstance(_[1], nn.Linear))\n",
    "        debug_print(f\"Found {linear_count} linear layers to convert to analog\")\n",
    "        \n",
    "        # Replace all linear layers with analog linear layers\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                debug_print(f\"Converting linear layer '{name}' to analog\")\n",
    "                # Create analog layer with same dimensions and bias configuration\n",
    "                analog_layer = AnalogLinear(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    bias=module.bias is not None,\n",
    "                    rpu_config=rpu_config\n",
    "                )\n",
    "                # Navigate to the parent module to replace the layer\n",
    "                parent = self\n",
    "                for n in name.split('.')[:-1]:\n",
    "                    parent = getattr(parent, n)\n",
    "                setattr(parent, name.split('.')[-1], analog_layer)\n",
    "        \n",
    "        debug_print(\"All linear layers converted to analog successfully\")\n",
    "        \n",
    "        # Fix the forced_decoder_ids issue as recommended in GitHub issues\n",
    "        if hasattr(self.generation_config, \"forced_decoder_ids\"):\n",
    "            debug_print(\"Converting forced_decoder_ids to input_ids in model's generation config\")\n",
    "            self.generation_config.input_ids = self.generation_config.forced_decoder_ids\n",
    "            self.generation_config.forced_decoder_ids = None\n",
    "\n",
    "    def transfer_digital_weights(self, digital_model):\n",
    "        \"\"\"\n",
    "        Transfer weights from digital model to analog model using from_digital method\n",
    "        This ensures proper weight transfer considering analog hardware characteristics\n",
    "        \"\"\"\n",
    "        debug_print(\"Starting weight transfer from digital to analog model...\")\n",
    "        \n",
    "        # Count the number of analog layers to transfer weights to\n",
    "        analog_count = sum(1 for _ in self.named_modules() if isinstance(_[1], AnalogLinear))\n",
    "        debug_print(f\"Found {analog_count} analog layers to transfer weights to\")\n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, AnalogLinear):\n",
    "                debug_print(f\"Transferring weights for analog layer '{name}'\")\n",
    "                # Get the corresponding digital module\n",
    "                digital_module = digital_model\n",
    "                for n in name.split('.'):\n",
    "                    digital_module = getattr(digital_module, n)\n",
    "                # Use self.rpu_config instead of rpu_config - THIS IS THE KEY FIX\n",
    "                module.from_digital(digital_module, rpu_config=self.rpu_config)\n",
    "        \n",
    "        debug_print(\"Weight transfer completed successfully\")\n",
    "\n",
    "''')\n",
    "\n",
    "print(\"whisper_analog.py file created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16c5b0-390d-4ad3-b5a1-974e580d244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.exceptions import TileModuleError\n",
    "from itertools import islice\n",
    "\n",
    "# Import your AnalogWhisperForConditionalGeneration class\n",
    "from whisper_analog import AnalogWhisperForConditionalGeneration\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    logger.warning(\"CUDA is not available. Falling back to CPU.\")\n",
    "    DEVICE = \"cpu\"\n",
    "else:\n",
    "    DEVICE = \"cuda\"\n",
    "    # Initialize CUDA\n",
    "    torch.cuda.init()\n",
    "    logger.info(f\"CUDA initialized. Using device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "class WhisperPerformanceEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"openai/whisper-tiny.en\",  # Default to tiny model\n",
    "        dataset_name: str = \"librispeech_asr\",\n",
    "        split: str = \"test.clean\",\n",
    "        batch_size: int = 4,\n",
    "        debug: bool = False,\n",
    "        use_mini_dataset: bool = True\n",
    "    ):\n",
    "        self.debug = debug\n",
    "        self.batch_size = batch_size\n",
    "        self.device = DEVICE\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load processor and model\n",
    "        logger.info(\"Loading processor and model...\")\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "        # Load digital model\n",
    "        logger.info(f\"Loading digital Whisper model: {model_name}\")\n",
    "        self.digital_model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.digital_model.eval()\n",
    "        self.digital_model = self.digital_model.to(self.device)\n",
    "\n",
    "        # Load analog model\n",
    "        logger.info(\"Creating and initializing analog Whisper model...\")\n",
    "        config = WhisperConfig.from_pretrained(model_name)\n",
    "        rpu_config = InferenceRPUConfig()\n",
    "        self.analog_model = AnalogWhisperForConditionalGeneration(\n",
    "            config,\n",
    "            rpu_config=rpu_config,\n",
    "            debug=debug\n",
    "        )\n",
    "        self.analog_model.transfer_digital_weights(self.digital_model)\n",
    "        \n",
    "        # Fix for Whisper forced_decoder_ids issue\n",
    "        if hasattr(self.analog_model.generation_config, \"forced_decoder_ids\"):\n",
    "            logger.info(\"Converting forced_decoder_ids to input_ids in model's generation config\")\n",
    "            self.analog_model.generation_config.input_ids = self.analog_model.generation_config.forced_decoder_ids\n",
    "            self.analog_model.generation_config.forced_decoder_ids = None\n",
    "            \n",
    "        self.analog_model.eval()\n",
    "        self.analog_model = self.analog_model.to(self.device)\n",
    "\n",
    "        # Load dataset\n",
    "        if use_mini_dataset:\n",
    "            logger.info(\"Loading Mini LibriSpeech dataset...\")\n",
    "            self.dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n",
    "            self.dataset = list(islice(self.dataset, 50))\n",
    "        else:\n",
    "            logger.info(f\"Loading dataset {dataset_name}...\")\n",
    "            self.dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "        logger.info(f\"Dataset size: {len(self.dataset)} samples\")\n",
    "        \n",
    "        # Create decoder input IDs for Whisper (needed for generation)\n",
    "        self.language = \"en\"\n",
    "        self.task = \"transcribe\"\n",
    "        self.language_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.language)\n",
    "        self.task_token_id = self.processor.tokenizer.convert_tokens_to_ids(self.task)\n",
    "\n",
    "    def custom_collate_fn(self, batch):\n",
    "        # Just return the batch as is, we'll handle the processing in preprocess_batch\n",
    "        return batch\n",
    "\n",
    "    def preprocess_batch(self, batch: Dict) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Extract audio arrays from the batch\n",
    "        audio_arrays = [item[\"audio\"][\"array\"] for item in batch]\n",
    "        \n",
    "        # Process the batch with padding\n",
    "        processed = self.processor(\n",
    "            audio_arrays,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Get input features (mel spectrograms)\n",
    "        input_features = processed.input_features\n",
    "        \n",
    "        # Ensure the features are exactly 3000 frames in length\n",
    "        batch_size, num_mels, seq_len = input_features.shape\n",
    "        if seq_len < 3000:\n",
    "            # Pad to 3000 frames (30 seconds)\n",
    "            padding = torch.zeros(batch_size, num_mels, 3000 - seq_len, device=input_features.device)\n",
    "            input_features = torch.cat([input_features, padding], dim=2)\n",
    "        elif seq_len > 3000:\n",
    "            # Truncate to 3000 frames (30 seconds)\n",
    "            input_features = input_features[:, :, :3000]\n",
    "        \n",
    "        # Create appropriate attention mask\n",
    "        attention_mask = torch.ones((batch_size, input_features.shape[2]), dtype=torch.long, device=input_features.device)\n",
    "        \n",
    "        return input_features, attention_mask\n",
    "\n",
    "\n",
    "    def calculate_wer(self, predictions: List[str], references: List[str]) -> float:\n",
    "        \"\"\"Calculate Word Error Rate between predictions and references\"\"\"\n",
    "        total_words = sum(len(ref.split()) for ref in references)\n",
    "        errors = 0\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Clean and normalize text for comparison\n",
    "            pred = pred.strip().lower()\n",
    "            ref = ref.strip().lower()\n",
    "            \n",
    "            pred_words = pred.split()\n",
    "            ref_words = ref.split()\n",
    "            \n",
    "            # Calculate Levenshtein distance (minimum edit operations)\n",
    "            from jiwer import wer\n",
    "            try:\n",
    "                current_wer = wer(ref, pred)\n",
    "                errors += current_wer * len(ref_words)\n",
    "            except:\n",
    "                # Fallback method if jiwer fails\n",
    "                distance = self.levenshtein_distance(pred_words, ref_words)\n",
    "                errors += distance\n",
    "                \n",
    "        return errors / total_words if total_words > 0 else 1.0\n",
    "\n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        \"\"\"Simple Levenshtein distance implementation for word sequences\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return self.levenshtein_distance(s2, s1)\n",
    "\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "\n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "\n",
    "        return previous_row[-1]\n",
    "\n",
    "    def direct_inference(self, model, input_features, attention_mask, decoder_input_ids, device=\"cuda\"):\n",
    "        \"\"\"Perform direct inference without using generate() if it's problematic\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use forward pass instead of generate\n",
    "            outputs = model(\n",
    "                input_features, \n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Get the predicted IDs from logits\n",
    "            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # Decode the predicted IDs to text\n",
    "            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            \n",
    "            return transcription\n",
    "\n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        dataloader: DataLoader,\n",
    "        device: str = \"cuda\",\n",
    "        use_generate: bool = True  # Option to use generate or direct inference\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate a model on the given dataloader and return performance metrics\"\"\"\n",
    "        model = model.to(device)\n",
    "        predictions = []\n",
    "        references = []\n",
    "        inference_times = []\n",
    "\n",
    "        # Create a custom generation config to handle forced_decoder_ids issue\n",
    "        generation_config = copy.deepcopy(model.generation_config)\n",
    "        if hasattr(generation_config, \"forced_decoder_ids\"):\n",
    "            generation_config.input_ids = generation_config.forced_decoder_ids\n",
    "            generation_config.forced_decoder_ids = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                # Preprocess batch - get both features and attention mask\n",
    "                input_features, attention_mask = self.preprocess_batch(batch)\n",
    "                input_features = input_features.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                \n",
    "                # Create decoder_input_ids with the right starting tokens\n",
    "                decoder_input_ids = torch.tensor([[\n",
    "                    self.processor.tokenizer.bos_token_id, \n",
    "                    self.language_token_id, \n",
    "                    self.task_token_id\n",
    "                ]]).to(device)\n",
    "\n",
    "                # Measure inference time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    if use_generate:\n",
    "                        # Generate predictions with attention mask and simplified parameters\n",
    "                        generated_ids = model.generate(\n",
    "                            input_features,\n",
    "                            attention_mask=attention_mask,  # Add attention mask\n",
    "                            decoder_input_ids=decoder_input_ids,\n",
    "                            generation_config=generation_config,\n",
    "                            max_length=128,\n",
    "                            num_beams=1,  # Simple greedy decoding for reliability\n",
    "                            return_dict_in_generate=True,  # Get the full output object\n",
    "                            output_scores=False,  # Don't need scores\n",
    "                            return_timestamps=False  # Don't return timestamps for simplicity\n",
    "                        ).sequences\n",
    "                        \n",
    "                        # Decode the generated IDs to text\n",
    "                        transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "                    else:\n",
    "                        # Use direct inference as fallback\n",
    "                        transcription = self.direct_inference(\n",
    "                            model, \n",
    "                            input_features, \n",
    "                            attention_mask, \n",
    "                            decoder_input_ids, \n",
    "                            device\n",
    "                        )\n",
    "                    \n",
    "                    inference_time = time.time() - start_time\n",
    "                    inference_times.append(inference_time)\n",
    "                    \n",
    "                    predictions.extend(transcription)\n",
    "                    # Extract reference text from each item in the batch\n",
    "                    references.extend([item[\"text\"] for item in batch])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during inference: {e}\")\n",
    "                    # Try fallback to direct inference if generate failed\n",
    "                    if use_generate:\n",
    "                        logger.info(\"Falling back to direct inference...\")\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "                            transcription = self.direct_inference(\n",
    "                                model, \n",
    "                                input_features, \n",
    "                                attention_mask, \n",
    "                                decoder_input_ids, \n",
    "                                device\n",
    "                            )\n",
    "                            inference_time = time.time() - start_time\n",
    "                            inference_times.append(inference_time)\n",
    "                            \n",
    "                            predictions.extend(transcription)\n",
    "                            references.extend([item[\"text\"] for item in batch])\n",
    "                        except Exception as e2:\n",
    "                            logger.error(f\"Fallback also failed: {e2}\")\n",
    "                            # Skip this batch\n",
    "                            continue\n",
    "                    else:\n",
    "                        # Skip this batch\n",
    "                        continue\n",
    "\n",
    "        # Calculate metrics only if we have predictions\n",
    "        if predictions:\n",
    "            wer = self.calculate_wer(predictions, references)\n",
    "            avg_inference_time = np.mean(inference_times)\n",
    "\n",
    "            # Print sample predictions\n",
    "            logger.info(\"Sample predictions:\")\n",
    "            for i in range(min(3, len(predictions))):\n",
    "                logger.info(f\"Reference: {references[i]}\")\n",
    "                logger.info(f\"Prediction: {predictions[i]}\")\n",
    "                logger.info(\"---\")\n",
    "\n",
    "            return {\n",
    "                \"wer\": wer,\n",
    "                \"avg_inference_time\": avg_inference_time,\n",
    "                \"total_samples\": len(predictions)\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(\"No successful predictions were generated\")\n",
    "            return {\n",
    "                \"wer\": 1.0,  # Maximum error\n",
    "                \"avg_inference_time\": 0.0,\n",
    "                \"total_samples\": 0\n",
    "            }\n",
    "\n",
    "    def run_comparison(self, num_samples: int = 20, use_generate: bool = True) -> Dict:\n",
    "        \"\"\"Run a comparison between digital and analog models\"\"\"\n",
    "        data = list(islice(self.dataset, num_samples))\n",
    "        dataloader = DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "        logger.info(\"Evaluating digital model...\")\n",
    "        digital_results = self.evaluate_model(self.digital_model, dataloader, use_generate=use_generate)\n",
    "\n",
    "        logger.info(\"Evaluating analog model...\")\n",
    "        analog_results = self.evaluate_model(self.analog_model, dataloader, use_generate=use_generate)\n",
    "\n",
    "        wer_diff = analog_results[\"wer\"] - digital_results[\"wer\"]\n",
    "        time_diff = analog_results[\"avg_inference_time\"] - digital_results[\"avg_inference_time\"]\n",
    "        \n",
    "        # Calculate acceleration and quality degradation percentages\n",
    "        time_speedup_pct = (digital_results[\"avg_inference_time\"] / max(0.001, analog_results[\"avg_inference_time\"]) - 1) * 100\n",
    "        wer_increase_pct = (analog_results[\"wer\"] / max(0.001, digital_results[\"wer\"]) - 1) * 100\n",
    "\n",
    "        return {\n",
    "            \"digital\": digital_results,\n",
    "            \"analog\": analog_results,\n",
    "            \"wer_difference\": wer_diff,\n",
    "            \"wer_increase_percent\": wer_increase_pct,\n",
    "            \"time_difference\": time_diff,\n",
    "            \"time_speedup_percent\": time_speedup_pct\n",
    "        }\n",
    "\n",
    "    def analyze_noise_sensitivity(\n",
    "        self,\n",
    "        noise_levels: List[float],\n",
    "        num_samples: int = 10,\n",
    "        use_generate: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"Analyze the impact of different noise levels on model performance\"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for noise_level in noise_levels:\n",
    "            logger.info(f\"Testing noise level: {noise_level}\")\n",
    "\n",
    "            # Create a new RPU config with specified noise level\n",
    "            rpu_config = InferenceRPUConfig()\n",
    "            rpu_config.forward.out_noise = noise_level  # Adjust noise parameter\n",
    "\n",
    "            # Create a new analog model with this noise level\n",
    "            noisy_model = AnalogWhisperForConditionalGeneration(\n",
    "                self.digital_model.config,\n",
    "                rpu_config=rpu_config,\n",
    "                debug=self.debug\n",
    "            )\n",
    "            noisy_model.transfer_digital_weights(self.digital_model)\n",
    "            \n",
    "            # Fix for Whisper forced_decoder_ids issue\n",
    "            if hasattr(noisy_model.generation_config, \"forced_decoder_ids\"):\n",
    "                noisy_model.generation_config.input_ids = noisy_model.generation_config.forced_decoder_ids\n",
    "                noisy_model.generation_config.forced_decoder_ids = None\n",
    "                \n",
    "            noisy_model.eval()\n",
    "\n",
    "            try:\n",
    "                noisy_model = noisy_model.to(self.device)\n",
    "            except TileModuleError as e:\n",
    "                logger.warning(f\"Analog model with noise could not use CUDA. Falling back to CPU. Reason: {e}\")\n",
    "                self.device = \"cpu\"\n",
    "                noisy_model = noisy_model.to(self.device)\n",
    "\n",
    "            data = list(islice(self.dataset, num_samples))\n",
    "            dataloader = DataLoader(\n",
    "                data,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=self.custom_collate_fn\n",
    "            )\n",
    "            results[noise_level] = self.evaluate_model(\n",
    "                noisy_model, \n",
    "                dataloader, \n",
    "                device=self.device,\n",
    "                use_generate=use_generate\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Print transformers version for debugging\n",
    "    import transformers\n",
    "    logger.info(f\"Using transformers version: {transformers.__version__}\")\n",
    "    \n",
    "    # You can choose different model sizes: \n",
    "    # \"openai/whisper-tiny.en\" (39M params)\n",
    "    # \"openai/whisper-base.en\" (74M params)\n",
    "    # \"openai/whisper-small.en\" (244M params)\n",
    "    evaluator = WhisperPerformanceEvaluator(\n",
    "        model_name=\"openai/whisper-tiny.en\",\n",
    "        debug=False, \n",
    "        use_mini_dataset=True,\n",
    "        batch_size=2  # Smaller batch size to avoid CUDA OOM\n",
    "    )\n",
    "\n",
    "    # Set to False to use direct inference instead of generate()\n",
    "    use_generate = False\n",
    "    \n",
    "    logger.info(f\"Running basic comparison between digital and analog Whisper models (use_generate={use_generate})...\")\n",
    "    comparison_results = evaluator.run_comparison(num_samples=10, use_generate=use_generate)\n",
    "    logger.info(\"==== DIGITAL VS ANALOG COMPARISON RESULTS ====\")\n",
    "    logger.info(f\"Digital WER: {comparison_results['digital']['wer']:.4f}\")\n",
    "    logger.info(f\"Analog WER: {comparison_results['analog']['wer']:.4f}\")\n",
    "    logger.info(f\"WER increase: {comparison_results['wer_increase_percent']:.2f}%\")\n",
    "    logger.info(f\"Digital inference time: {comparison_results['digital']['avg_inference_time']:.4f}s\")\n",
    "    logger.info(f\"Analog inference time: {comparison_results['analog']['avg_inference_time']:.4f}s\")\n",
    "    logger.info(f\"Speed change: {comparison_results['time_speedup_percent']:.2f}%\")\n",
    "    logger.info(\"============================================\")\n",
    "\n",
    "    # Uncomment to run noise sensitivity analysis\n",
    "    # logger.info(\"Running noise sensitivity analysis...\")\n",
    "    # noise_levels = [0.0, 0.05, 0.1, 0.2]\n",
    "    # noise_results = evaluator.analyze_noise_sensitivity(noise_levels, num_samples=5, use_generate=use_generate)\n",
    "    # logger.info(\"==== NOISE SENSITIVITY RESULTS ====\")\n",
    "    # for level, result in noise_results.items():\n",
    "    #     logger.info(f\"Noise level {level}: WER = {result['wer']:.4f}, Inference time = {result['avg_inference_time']:.4f}s\")\n",
    "    # logger.info(\"===================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19764a1f-d9a3-46e1-88e1-1502c305e25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6bc27-0855-4133-b5df-8efb2cce2f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c93b06-df31-4199-ac7a-086a115301e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a4822-9f59-43ad-804c-f517e512ebab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26c917-8432-4a3a-b90b-ee405257a4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434d0f6-45d9-4379-aeaf-dc75b9e70f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550c915-d439-48a3-976a-54178e252db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d2a3e-3b7d-49ee-bbe5-41910051c603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5cca5ee-6060-4629-a05b-bebff7969fa1",
   "metadata": {},
   "source": [
    "### New Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72d19d-ed56-45c8-b2af-489368808865",
   "metadata": {},
   "source": [
    "### OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d830ea7-d5f2-453b-8f60-bef67f718e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/tgs2126\n",
      "Available audio files:\n",
      "1. audio.wav\n",
      "Using audio file: /home/tgs2126/audio.wav\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import whisper\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Option 1: List all audio files in the current directory\n",
    "audio_extensions = ['.mp3', '.wav', '.flac', '.m4a', '.ogg']\n",
    "audio_files = []\n",
    "\n",
    "for file in os.listdir(current_dir):\n",
    "    # Check if the file has an audio extension\n",
    "    if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
    "        audio_files.append(file)\n",
    "\n",
    "# Print available audio files\n",
    "if audio_files:\n",
    "    print(\"Available audio files:\")\n",
    "    for i, file in enumerate(audio_files):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "    \n",
    "    selected_file = audio_files[0]  # Use the first audio file by default\n",
    "    # selected_file = input(\"Enter the number or name of the file you want to use: \")\n",
    "    \n",
    "    # Handle numeric selection\n",
    "    if selected_file.isdigit() and 1 <= int(selected_file) <= len(audio_files):\n",
    "        selected_file = audio_files[int(selected_file)-1]\n",
    "    \n",
    "    # Create full path to the audio file\n",
    "    audio_path = os.path.join(current_dir, selected_file)\n",
    "    print(f\"Using audio file: {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9cb46c-bb48-4086-9d77-501ea34835a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper 'small' model...\n",
      "Model loaded.\n",
      "\n",
      "Running full pipeline (audio loading + inference) 100 times...\n",
      "Run 1: 1.261 seconds, Detected language: en\n",
      "Run 2: 1.157 seconds, Detected language: en\n",
      "Run 3: 1.168 seconds, Detected language: en\n",
      "Run 4: 1.167 seconds, Detected language: en\n",
      "Run 5: 1.174 seconds, Detected language: en\n",
      "Run 6: 1.161 seconds, Detected language: en\n",
      "Run 7: 1.159 seconds, Detected language: en\n",
      "Run 8: 1.166 seconds, Detected language: en\n",
      "Run 9: 1.180 seconds, Detected language: en\n",
      "Run 10: 1.183 seconds, Detected language: en\n",
      "Run 11: 1.185 seconds, Detected language: en\n",
      "Run 12: 1.169 seconds, Detected language: en\n",
      "Run 13: 1.166 seconds, Detected language: en\n",
      "Run 14: 1.176 seconds, Detected language: en\n",
      "Run 15: 1.172 seconds, Detected language: en\n",
      "Run 16: 1.173 seconds, Detected language: en\n",
      "Run 17: 1.166 seconds, Detected language: en\n",
      "Run 18: 1.179 seconds, Detected language: en\n",
      "Run 19: 1.176 seconds, Detected language: en\n",
      "Run 20: 1.179 seconds, Detected language: en\n",
      "Run 21: 1.174 seconds, Detected language: en\n",
      "Run 22: 1.166 seconds, Detected language: en\n",
      "Run 23: 1.161 seconds, Detected language: en\n",
      "Run 24: 1.171 seconds, Detected language: en\n",
      "Run 25: 1.152 seconds, Detected language: en\n",
      "Run 26: 1.172 seconds, Detected language: en\n",
      "Run 27: 1.168 seconds, Detected language: en\n",
      "Run 28: 1.155 seconds, Detected language: en\n",
      "Run 29: 1.166 seconds, Detected language: en\n",
      "Run 30: 1.159 seconds, Detected language: en\n",
      "Run 31: 1.190 seconds, Detected language: en\n",
      "Run 32: 1.201 seconds, Detected language: en\n",
      "Run 33: 1.155 seconds, Detected language: en\n",
      "Run 34: 1.169 seconds, Detected language: en\n",
      "Run 35: 1.192 seconds, Detected language: en\n",
      "Run 36: 1.185 seconds, Detected language: en\n",
      "Run 37: 1.182 seconds, Detected language: en\n",
      "Run 38: 1.231 seconds, Detected language: en\n",
      "Run 39: 1.196 seconds, Detected language: en\n",
      "Run 40: 1.184 seconds, Detected language: en\n",
      "Run 41: 1.177 seconds, Detected language: en\n",
      "Run 42: 1.195 seconds, Detected language: en\n",
      "Run 43: 1.180 seconds, Detected language: en\n",
      "Run 44: 1.171 seconds, Detected language: en\n",
      "Run 45: 1.187 seconds, Detected language: en\n",
      "Run 46: 1.165 seconds, Detected language: en\n",
      "Run 47: 1.209 seconds, Detected language: en\n",
      "Run 48: 1.175 seconds, Detected language: en\n",
      "Run 49: 1.175 seconds, Detected language: en\n",
      "Run 50: 1.187 seconds, Detected language: en\n",
      "Run 51: 1.188 seconds, Detected language: en\n",
      "Run 52: 1.182 seconds, Detected language: en\n",
      "Run 53: 1.195 seconds, Detected language: en\n",
      "Run 54: 1.190 seconds, Detected language: en\n",
      "Run 55: 1.195 seconds, Detected language: en\n",
      "Run 56: 1.186 seconds, Detected language: en\n",
      "Run 57: 1.173 seconds, Detected language: en\n",
      "Run 58: 1.191 seconds, Detected language: en\n",
      "Run 59: 1.185 seconds, Detected language: en\n",
      "Run 60: 1.180 seconds, Detected language: en\n",
      "Run 61: 1.176 seconds, Detected language: en\n",
      "Run 62: 1.186 seconds, Detected language: en\n",
      "Run 63: 1.193 seconds, Detected language: en\n",
      "Run 64: 1.189 seconds, Detected language: en\n",
      "Run 65: 1.194 seconds, Detected language: en\n",
      "Run 66: 1.192 seconds, Detected language: en\n",
      "Run 67: 1.182 seconds, Detected language: en\n",
      "Run 68: 1.176 seconds, Detected language: en\n",
      "Run 69: 1.180 seconds, Detected language: en\n",
      "Run 70: 1.180 seconds, Detected language: en\n",
      "Run 71: 1.170 seconds, Detected language: en\n",
      "Run 72: 1.176 seconds, Detected language: en\n",
      "Run 73: 1.181 seconds, Detected language: en\n",
      "Run 74: 1.166 seconds, Detected language: en\n",
      "Run 75: 1.166 seconds, Detected language: en\n",
      "Run 76: 1.175 seconds, Detected language: en\n",
      "Run 77: 1.193 seconds, Detected language: en\n",
      "Run 78: 1.171 seconds, Detected language: en\n",
      "Run 79: 1.150 seconds, Detected language: en\n",
      "Run 80: 1.156 seconds, Detected language: en\n",
      "Run 81: 1.170 seconds, Detected language: en\n",
      "Run 82: 1.199 seconds, Detected language: en\n",
      "Run 83: 1.204 seconds, Detected language: en\n",
      "Run 84: 1.207 seconds, Detected language: en\n",
      "Run 85: 1.179 seconds, Detected language: en\n",
      "Run 86: 1.182 seconds, Detected language: en\n",
      "Run 87: 1.197 seconds, Detected language: en\n",
      "Run 88: 1.164 seconds, Detected language: en\n",
      "Run 89: 1.176 seconds, Detected language: en\n",
      "Run 90: 1.169 seconds, Detected language: en\n",
      "Run 91: 1.173 seconds, Detected language: en\n",
      "Run 92: 1.181 seconds, Detected language: en\n",
      "Run 93: 1.186 seconds, Detected language: en\n",
      "Run 94: 1.175 seconds, Detected language: en\n",
      "Run 95: 1.163 seconds, Detected language: en\n",
      "Run 96: 1.150 seconds, Detected language: en\n",
      "Run 97: 1.187 seconds, Detected language: en\n",
      "Run 98: 1.183 seconds, Detected language: en\n",
      "Run 99: 1.177 seconds, Detected language: en\n",
      "Run 100: 1.175 seconds, Detected language: en\n",
      "\n",
      "Average full pipeline runtime over 100 runs: 1.179 seconds\n",
      "\n",
      "Recognized text from the last run:\n",
      "The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import whisper\n",
    "\n",
    "# Load the Whisper \"small\" model once (this is outside the loop to avoid reloading each time)\n",
    "print(\"Loading Whisper 'small' model...\")\n",
    "model = whisper.load_model(\"small\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Define number of runs and an array to accumulate elapsed time\n",
    "n_runs = 100\n",
    "times = []\n",
    "last_result = None\n",
    "\n",
    "print(f\"\\nRunning full pipeline (audio loading + inference) {n_runs} times...\")\n",
    "\n",
    "# Loop over runs\n",
    "for i in range(n_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Audio Loading and Preprocessing ---\n",
    "    # Load the audio file\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    # Pad or trim the audio to 30 seconds (default behavior)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    # Generate log-mel spectrogram and move to the model's device\n",
    "    mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "    \n",
    "    # --- Inference ---\n",
    "    # Detect spoken language (optional)\n",
    "    _, probs = model.detect_language(mel)\n",
    "    detected_language = max(probs, key=probs.get)\n",
    "    \n",
    "    # Decode the audio using default decoding options\n",
    "    options = whisper.DecodingOptions()\n",
    "    last_result = whisper.decode(model, mel, options)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    times.append(elapsed)\n",
    "    print(f\"Run {i+1}: {elapsed:.3f} seconds, Detected language: {detected_language}\")\n",
    "\n",
    "# Compute and print the average time over all runs\n",
    "avg_time = sum(times) / n_runs\n",
    "print(f\"\\nAverage full pipeline runtime over {n_runs} runs: {avg_time:.3f} seconds\")\n",
    "\n",
    "# Print the recognized text from the last run\n",
    "print(\"\\nRecognized text from the last run:\")\n",
    "print(last_result.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964ef7d-6e36-404a-90ef-b1aacd03713d",
   "metadata": {},
   "source": [
    "### IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f14062-c0e6-4753-bf73-279ad4b128b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading digital Whisper 'small' model...\n",
      "Digital model loaded.\n",
      "Converting model to analog...\n",
      "Analog conversion complete.\n",
      "\n",
      "Running full pipeline (audio loading, preprocessing & inference) 100 times...\n",
      "Run 1: 1.287 seconds, Detected language: en\n",
      "Run 2: 1.199 seconds, Detected language: en\n",
      "Run 3: 1.166 seconds, Detected language: en\n",
      "Run 4: 1.194 seconds, Detected language: en\n",
      "Run 5: 1.164 seconds, Detected language: en\n",
      "Run 6: 1.171 seconds, Detected language: en\n",
      "Run 7: 1.190 seconds, Detected language: en\n",
      "Run 8: 1.194 seconds, Detected language: en\n",
      "Run 9: 1.169 seconds, Detected language: en\n",
      "Run 10: 1.179 seconds, Detected language: en\n",
      "Run 11: 1.187 seconds, Detected language: en\n",
      "Run 12: 1.175 seconds, Detected language: en\n",
      "Run 13: 1.204 seconds, Detected language: en\n",
      "Run 14: 1.181 seconds, Detected language: en\n",
      "Run 15: 1.207 seconds, Detected language: en\n",
      "Run 16: 1.200 seconds, Detected language: en\n",
      "Run 17: 1.233 seconds, Detected language: en\n",
      "Run 18: 1.203 seconds, Detected language: en\n",
      "Run 19: 1.181 seconds, Detected language: en\n",
      "Run 20: 1.184 seconds, Detected language: en\n",
      "Run 21: 1.188 seconds, Detected language: en\n",
      "Run 22: 1.176 seconds, Detected language: en\n",
      "Run 23: 1.193 seconds, Detected language: en\n",
      "Run 24: 1.185 seconds, Detected language: en\n",
      "Run 25: 1.180 seconds, Detected language: en\n",
      "Run 26: 1.191 seconds, Detected language: en\n",
      "Run 27: 1.194 seconds, Detected language: en\n",
      "Run 28: 1.183 seconds, Detected language: en\n",
      "Run 29: 1.188 seconds, Detected language: en\n",
      "Run 30: 1.185 seconds, Detected language: en\n",
      "Run 31: 1.182 seconds, Detected language: en\n",
      "Run 32: 1.206 seconds, Detected language: en\n",
      "Run 33: 1.191 seconds, Detected language: en\n",
      "Run 34: 1.166 seconds, Detected language: en\n",
      "Run 35: 1.156 seconds, Detected language: en\n",
      "Run 36: 1.184 seconds, Detected language: en\n",
      "Run 37: 1.193 seconds, Detected language: en\n",
      "Run 38: 1.184 seconds, Detected language: en\n",
      "Run 39: 1.188 seconds, Detected language: en\n",
      "Run 40: 1.182 seconds, Detected language: en\n",
      "Run 41: 1.198 seconds, Detected language: en\n",
      "Run 42: 1.182 seconds, Detected language: en\n",
      "Run 43: 1.194 seconds, Detected language: en\n",
      "Run 44: 1.188 seconds, Detected language: en\n",
      "Run 45: 1.161 seconds, Detected language: en\n",
      "Run 46: 1.176 seconds, Detected language: en\n",
      "Run 47: 1.191 seconds, Detected language: en\n",
      "Run 48: 1.187 seconds, Detected language: en\n",
      "Run 49: 1.191 seconds, Detected language: en\n",
      "Run 50: 1.196 seconds, Detected language: en\n",
      "Run 51: 1.173 seconds, Detected language: en\n",
      "Run 52: 1.192 seconds, Detected language: en\n",
      "Run 53: 1.186 seconds, Detected language: en\n",
      "Run 54: 1.201 seconds, Detected language: en\n",
      "Run 55: 1.176 seconds, Detected language: en\n",
      "Run 56: 1.195 seconds, Detected language: en\n",
      "Run 57: 1.184 seconds, Detected language: en\n",
      "Run 58: 1.190 seconds, Detected language: en\n",
      "Run 59: 1.186 seconds, Detected language: en\n",
      "Run 60: 1.190 seconds, Detected language: en\n",
      "Run 61: 1.193 seconds, Detected language: en\n",
      "Run 62: 1.211 seconds, Detected language: en\n",
      "Run 63: 1.195 seconds, Detected language: en\n",
      "Run 64: 1.194 seconds, Detected language: en\n",
      "Run 65: 1.183 seconds, Detected language: en\n",
      "Run 66: 1.240 seconds, Detected language: en\n",
      "Run 67: 1.196 seconds, Detected language: en\n",
      "Run 68: 1.202 seconds, Detected language: en\n",
      "Run 69: 1.186 seconds, Detected language: en\n",
      "Run 70: 1.185 seconds, Detected language: en\n",
      "Run 71: 1.191 seconds, Detected language: en\n",
      "Run 72: 1.192 seconds, Detected language: en\n",
      "Run 73: 1.177 seconds, Detected language: en\n",
      "Run 74: 1.184 seconds, Detected language: en\n",
      "Run 75: 1.185 seconds, Detected language: en\n",
      "Run 76: 1.202 seconds, Detected language: en\n",
      "Run 77: 1.174 seconds, Detected language: en\n",
      "Run 78: 1.204 seconds, Detected language: en\n",
      "Run 79: 1.198 seconds, Detected language: en\n",
      "Run 80: 1.190 seconds, Detected language: en\n",
      "Run 81: 1.195 seconds, Detected language: en\n",
      "Run 82: 1.182 seconds, Detected language: en\n",
      "Run 83: 1.250 seconds, Detected language: en\n",
      "Run 84: 1.199 seconds, Detected language: en\n",
      "Run 85: 1.189 seconds, Detected language: en\n",
      "Run 86: 1.148 seconds, Detected language: en\n",
      "Run 87: 1.191 seconds, Detected language: en\n",
      "Run 88: 1.190 seconds, Detected language: en\n",
      "Run 89: 1.182 seconds, Detected language: en\n",
      "Run 90: 1.205 seconds, Detected language: en\n",
      "Run 91: 1.173 seconds, Detected language: en\n",
      "Run 92: 1.186 seconds, Detected language: en\n",
      "Run 93: 1.180 seconds, Detected language: en\n",
      "Run 94: 1.190 seconds, Detected language: en\n",
      "Run 95: 1.185 seconds, Detected language: en\n",
      "Run 96: 1.184 seconds, Detected language: en\n",
      "Run 97: 1.184 seconds, Detected language: en\n",
      "Run 98: 1.169 seconds, Detected language: en\n",
      "Run 99: 1.188 seconds, Detected language: en\n",
      "Run 100: 1.201 seconds, Detected language: en\n",
      "\n",
      "Average total pipeline runtime over 100 runs: 1.190 seconds\n",
      "\n",
      "Recognized text from the last run:\n",
      "The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import whisper\n",
    "import torch\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.nn.conversion import convert_to_analog  # Import the conversion utility\n",
    "\n",
    "# --- Load the digital Whisper model ---\n",
    "print(\"Loading digital Whisper 'small' model...\")\n",
    "model = whisper.load_model(\"small\")\n",
    "print(\"Digital model loaded.\")\n",
    "\n",
    "# --- Create RPU configuration for analog inference ---\n",
    "rpu_config = InferenceRPUConfig()\n",
    "\n",
    "# --- Convert the model to analog using the convert_to_analog utility ---\n",
    "print(\"Converting model to analog...\")\n",
    "# Convert the entire model at once using the utility function\n",
    "analog_model = convert_to_analog(model, rpu_config)\n",
    "print(\"Analog conversion complete.\")\n",
    "\n",
    "# --- Benchmark the full pipeline ---\n",
    "n_runs = 100\n",
    "times = []\n",
    "last_result = None\n",
    "\n",
    "print(f\"\\nRunning full pipeline (audio loading, preprocessing & inference) {n_runs} times...\")\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Audio Loading and Preprocessing:\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(analog_model.device)\n",
    "    \n",
    "    # Inference with the analog model:\n",
    "    _, probs = analog_model.detect_language(mel)\n",
    "    detected_language = max(probs, key=probs.get)\n",
    "    options = whisper.DecodingOptions()  # default options\n",
    "    last_result = whisper.decode(analog_model, mel, options)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    times.append(elapsed)\n",
    "    print(f\"Run {i+1}: {elapsed:.3f} seconds, Detected language: {detected_language}\")\n",
    "\n",
    "avg_time = sum(times) / n_runs\n",
    "print(f\"\\nAverage total pipeline runtime over {n_runs} runs: {avg_time:.3f} seconds\")\n",
    "print(\"\\nRecognized text from the last run:\")\n",
    "print(last_result.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (benim)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
