{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93729611-4746-4567-b11d-ee98b7ef5f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import platform\n",
    "import time\n",
    "import wandb \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import json, pprint\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bf259-1f56-444d-92fb-ed497ee4ece4",
   "metadata": {},
   "source": [
    "# Wandb Benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b099431b-6b98-4b21-af0b-3b2bdba5cc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.15\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU model: Tesla T4\n",
      "Number of GPUs: 1\n",
      "Available GPU memory: 15.64 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No Cuda!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301e9804-6826-437b-904e-873beeb6cf92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 01:42:58,650 - INFO - Using primary device for stats: cuda\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mym3064\u001b[0m (\u001b[33mhpml_final_project\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ym3064/wandb/run-20250508_014300-ttcwgffd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled/runs/ttcwgffd' target=\"_blank\">Eval_distilled_lora_prepruned_vs_4bit_UserLoad</a></strong> to <a href='https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled' target=\"_blank\">https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled/runs/ttcwgffd' target=\"_blank\">https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled/runs/ttcwgffd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 01:43:01,367 - INFO - Weights & Biases initialized successfully.\n",
      "2025-05-08 01:43:01,368 - INFO - Loading data...\n",
      "2025-05-08 01:43:06,959 - INFO - Using tokenizer: gpt2 with padding_side='right'\n",
      "2025-05-08 01:43:06,969 - INFO - Data loaded. Val/Test texts: 2461\n",
      "2025-05-08 01:43:06,971 - INFO - \n",
      "===== Evaluating: Distilled LoRA Pre-Pruned (Loaded FP) from ./saved_models/distilled_lora_prepruned =====\n",
      "2025-05-08 01:43:06,973 - INFO - Loading model as GPT2LMHeadModel and moving to device: cuda...\n",
      "2025-05-08 01:43:10,293 - INFO - Successfully loaded Distilled LoRA Pre-Pruned (Loaded FP)\n",
      "2025-05-08 01:43:10,294 - INFO - Model type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "2025-05-08 01:43:35,510 - INFO - Distilled LoRA Pre-Pruned (Loaded FP) - Final Validation Perplexity: 8.35\n",
      "2025-05-08 01:43:35,511 - INFO - Running inference benchmark for Distilled LoRA Pre-Pruned (Loaded FP)...\n",
      "2025-05-08 01:43:35,513 - INFO - --- Starting Inference Benchmark (Generation: False) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=False:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 01:43:43,123 - INFO - --- Finished Inference Benchmark (Generation: False) ---\n",
      "2025-05-08 01:43:43,126 - INFO - --- Starting Inference Benchmark (Generation: True) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=True:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 50256}. If this is not desired, please set these values explicitly.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-05-08 01:43:48,360 - INFO - --- Finished Inference Benchmark (Generation: True) ---\n",
      "2025-05-08 01:43:48,605 - INFO - \n",
      "===== Evaluating: Distilled LoRA Pre-Pruned (Loaded Quant 4-bit) from ./saved_models/distilled_lora_prepruned =====\n",
      "2025-05-08 01:43:48,606 - INFO - Loading model with 4-bit quantization and device_map='auto'...\n",
      "2025-05-08 01:43:48,780 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-05-08 01:43:49,412 - INFO - Successfully loaded Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)\n",
      "2025-05-08 01:43:49,414 - INFO - Model type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "2025-05-08 01:44:00,208 - INFO - Distilled LoRA Pre-Pruned (Loaded Quant 4-bit) - Final Validation Perplexity: 8.46\n",
      "2025-05-08 01:44:00,209 - INFO - Running inference benchmark for Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)...\n",
      "2025-05-08 01:44:00,211 - INFO - --- Starting Inference Benchmark (Generation: False) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=False:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 01:44:03,228 - INFO - --- Finished Inference Benchmark (Generation: False) ---\n",
      "2025-05-08 01:44:03,231 - INFO - --- Starting Inference Benchmark (Generation: True) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=True:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-05-08 01:44:06,677 - INFO - --- Finished Inference Benchmark (Generation: True) ---\n",
      "2025-05-08 01:44:06,915 - INFO - \n",
      "===== Final Evaluation Comparison =====\n",
      "2025-05-08 01:44:06,939 - INFO - \n",
      "Comparison DataFrame:\n",
      "                                               Final Val PPL Peak GPU Mem (MB) Eval Fwd Latency (ms) Fwd TP (samples/s) Gen Latency (ms) Gen TP (samples/s)\n",
      "Distilled LoRA Pre-Pruned (Loaded FP)                   8.35                2,112.7              8.7              115.4             12.3               81.5\n",
      "Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)          8.46                1,537.0              3.0              330.4              7.8              127.6\n",
      "2025-05-08 01:44:07,527 - INFO - Comparison table logged to Weights & Biases.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/final_ppl</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/peak_mem_mb_eval</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/final_ppl</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/peak_mem_mb_eval</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/final_ppl</td><td>8.3456</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_latency_ms</td><td>8.6675</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_throughput</td><td>115.37356</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_latency_ms</td><td>12.2703</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_throughput</td><td>81.49762</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/peak_mem_mb_eval</td><td>2112.68164</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/final_ppl</td><td>8.45785</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_latency_ms</td><td>3.02657</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_throughput</td><td>330.40694</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_latency_ms</td><td>7.83654</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_throughput</td><td>127.6074</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/peak_mem_mb_eval</td><td>1537.04248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Eval_distilled_lora_prepruned_vs_4bit_UserLoad</strong> at: <a href='https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled/runs/ttcwgffd' target=\"_blank\">https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled/runs/ttcwgffd</a><br> View project at: <a href='https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled' target=\"_blank\">https://wandb.ai/hpml_final_project/Quantized%20and%20Flash%20Enabled</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250508_014300-ttcwgffd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 01:44:10,158 - INFO - Weights & Biases run finished.\n",
      "2025-05-08 01:44:10,159 - INFO - \n",
      "===== Script Finished =====\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time, math, logging\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import gc\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json # Not strictly needed in this version but often useful with adapters\n",
    "from transformers import GPT2Config   \n",
    "original_base_model_name_for_tokenizer = \"gpt2\" # Tokenizer should ideally match what was saved with merged_fp16\n",
    "flash_cfg = GPT2Config.from_pretrained(original_base_model_name_for_tokenizer)\n",
    "flash_cfg.use_flash_attention = True     \n",
    "\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using primary device for stats: {device}\")\n",
    "\n",
    "model_path_to_evaluate = \"./saved_models/distilled_lora_prepruned\"    \n",
    "original_base_model_name_for_tokenizer = \"gpt2\" # Tokenizer should ideally match what was saved with merged_fp16\n",
    "max_length = 128\n",
    "inference_batch_size = 16\n",
    "num_inference_batches = 50\n",
    "run_inference_benchmark = True\n",
    "\n",
    "bnb_config_for_4bit_quantized_load = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        project=\"Quantized and Flash Enabled\", # Updated project\n",
    "        name=f\"Eval_{os.path.basename(model_path_to_evaluate)}_vs_4bit_UserLoad\",\n",
    "        config={\n",
    "            \"model_path_evaluated\": model_path_to_evaluate,\n",
    "            \"original_base_model_name_for_tokenizer\": original_base_model_name_for_tokenizer,\n",
    "            \"max_length\": max_length,\n",
    "            \"inference_batch_size\": inference_batch_size,\n",
    "            \"num_inference_batches\": num_inference_batches,\n",
    "            \"run_inference_benchmark\": run_inference_benchmark,\n",
    "            \"bnb_config_for_4bit_version\": bnb_config_for_4bit_quantized_load.to_dict()\n",
    "        }\n",
    "    )\n",
    "    logger.info(\"Weights & Biases initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Weights & Biases: {e}\")\n",
    "    run = None\n",
    "\n",
    "logger.info(\"Loading data...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(original_base_model_name_for_tokenizer)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "logger.info(f\"Using tokenizer: {tokenizer.name_or_path} with padding_side='{tokenizer.padding_side}'\")\n",
    "\n",
    "val_texts_full = [t for t in dataset[\"validation\"][\"text\"] if t.strip()]\n",
    "test_texts_full = val_texts_full[:inference_batch_size * num_inference_batches]\n",
    "logger.info(f\"Data loaded. Val/Test texts: {len(val_texts_full)}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model, tokenizer, texts, device, batch_size=8, max_length=128):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    total_evaluated = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        if not batch: continue\n",
    "        total_evaluated += len(batch)\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs, labels=inputs.input_ids)\n",
    "        if hasattr(outputs, 'loss') and outputs.loss is not None:\n",
    "            losses.append(outputs.loss.item() * len(batch))\n",
    "    if not losses or total_evaluated == 0: return float('inf')\n",
    "    avg_loss = sum(losses) / total_evaluated\n",
    "    if avg_loss <= 0: return float('inf')\n",
    "    return math.exp(avg_loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_inference(model, tokenizer, texts, eval_device_ignored, batch_size=8, max_length=128, num_batches=50, generation=False):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    total_samples = 0\n",
    "    logger.info(f\"--- Starting Inference Benchmark (Generation: {generation}) ---\")\n",
    "    generation_config = GenerationConfig(max_new_tokens=5, pad_token_id=tokenizer.pad_token_id, eos_token_id=model.config.eos_token_id if hasattr(model.config, 'eos_token_id') else tokenizer.eos_token_id) if generation else None\n",
    "    \n",
    "    for i in tqdm(range(0, min(len(texts), batch_size * num_batches), batch_size), desc=f\"Inference Gen={generation}\", leave=False):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        if not batch_texts: continue\n",
    "        \n",
    "        model_input_device = next(model.parameters()).device\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(model_input_device)\n",
    "        \n",
    "        batch_samples = inputs['input_ids'].shape[0]; total_samples += batch_samples\n",
    "        start_time = time.perf_counter()\n",
    "        if generation: _ = model.generate(**inputs, generation_config=generation_config)\n",
    "        else: _ = model(**inputs)\n",
    "        if model_input_device.type == \"cuda\": torch.cuda.synchronize(model_input_device)\n",
    "        end_time = time.perf_counter(); latencies.append(end_time - start_time)\n",
    "    if not latencies: return {\"avg_inference_latency_ms_per_sample\": float('nan'), \"avg_inference_throughput_samples_sec\": float('nan')}\n",
    "    total_time_secs = sum(latencies)\n",
    "    throughput_samples_sec = total_samples / total_time_secs if total_time_secs > 0 else 0\n",
    "    avg_latency_sample = (total_time_secs / total_samples) * 1000 if total_samples > 0 else 0\n",
    "    logger.info(f\"--- Finished Inference Benchmark (Generation: {generation}) ---\")\n",
    "    return {\"avg_inference_latency_ms_per_sample\": avg_latency_sample, \"avg_inference_throughput_samples_sec\": throughput_samples_sec}\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "def evaluate_model_version(model_load_path, model_label, load_quantized, bnb_config=None):\n",
    "    logger.info(f\"\\n===== Evaluating: {model_label} from {model_load_path} =====\")\n",
    "    model = None\n",
    "    eval_results = {}\n",
    "    if device.type == \"cuda\": torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(model_load_path):\n",
    "            raise FileNotFoundError(f\"Model path does not exist: {model_load_path}\")\n",
    "\n",
    "        if load_quantized and bnb_config:\n",
    "            logger.info(\"Loading model with 4-bit quantization and device_map='auto'...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_load_path,\n",
    "                quantization_config=bnb_config,\n",
    "                config=flash_cfg, \n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"Loading model as GPT2LMHeadModel and moving to device: {device}...\")\n",
    "            model = GPT2LMHeadModel.from_pretrained(model_load_path,config=flash_cfg).to(device)\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {model_label}\")\n",
    "        logger.info(f\"Model type: {type(model)}\")\n",
    "        \n",
    "        eval_device_for_model = next(model.parameters()).device if next(model.parameters(), None) is not None else device\n",
    "\n",
    "        ppl = compute_perplexity(model, tokenizer, val_texts_full, eval_device_for_model,\n",
    "                                   batch_size=inference_batch_size, max_length=max_length)\n",
    "        eval_results[\"final_ppl\"] = ppl\n",
    "        logger.info(f\"{model_label} - Final Validation Perplexity: {ppl:.2f}\")\n",
    "\n",
    "        if run_inference_benchmark:\n",
    "            logger.info(f\"Running inference benchmark for {model_label}...\")\n",
    "            inference_res_fwd = benchmark_inference(model, tokenizer, test_texts_full, eval_device_for_model, batch_size=inference_batch_size, max_length=max_length, num_batches=num_inference_batches, generation=False)\n",
    "            inference_res_gen = benchmark_inference(model, tokenizer, test_texts_full, eval_device_for_model, batch_size=inference_batch_size, max_length=max_length, num_batches=num_inference_batches // 2, generation=True)\n",
    "            eval_results.update({\n",
    "                 \"fwd_pass_latency_ms\": inference_res_fwd[\"avg_inference_latency_ms_per_sample\"],\n",
    "                 \"fwd_pass_throughput\": inference_res_fwd[\"avg_inference_throughput_samples_sec\"],\n",
    "                 \"gen_latency_ms\": inference_res_gen[\"avg_inference_latency_ms_per_sample\"],\n",
    "                 \"gen_throughput\": inference_res_gen[\"avg_inference_throughput_samples_sec\"]})\n",
    "        \n",
    "        current_mem_eval = 0\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize(); current_mem_eval = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "        eval_results[\"peak_mem_mb_eval\"] = current_mem_eval\n",
    "        \n",
    "        all_results[model_label] = eval_results\n",
    "        if run and 'final_ppl' in eval_results and not math.isinf(eval_results['final_ppl']):\n",
    "            wandb.log({f\"Summary/{model_label}/{k}\": v for k, v in eval_results.items()})\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during {model_label} evaluation: {e}\")\n",
    "        all_results[model_label] = {\"final_ppl\": float('inf'), \"peak_mem_mb_eval\": \"Error\"}\n",
    "    finally:\n",
    "        if model is not None: del model\n",
    "        gc.collect(); torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "\n",
    "evaluate_model_version(\n",
    "    model_load_path=model_path_to_evaluate,\n",
    "    model_label=\"Distilled LoRA Pre-Pruned (Loaded FP)\", \n",
    "    load_quantized=False\n",
    ")\n",
    "\n",
    "evaluate_model_version(\n",
    "    model_load_path=model_path_to_evaluate, \n",
    "    model_label=\"Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)\",\n",
    "    load_quantized=True,\n",
    "    bnb_config=bnb_config_for_4bit_quantized_load\n",
    ")\n",
    "\n",
    "logger.info(\"\\n===== Final Evaluation Comparison =====\")\n",
    "results_list_df = []\n",
    "indices_df = []\n",
    "ordered_labels = [\n",
    "    \"Distilled LoRA Pre-Pruned (Loaded FP)\",\n",
    "    \"Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)\"\n",
    "]\n",
    "for label in ordered_labels:\n",
    "    if label in all_results:\n",
    "        results_list_df.append(all_results[label])\n",
    "        indices_df.append(label)\n",
    "\n",
    "if results_list_df:\n",
    "    df = pd.DataFrame(results_list_df, index=indices_df)\n",
    "    cols_to_rename = {\"peak_mem_mb_eval\": \"Peak GPU Mem (MB) Eval\", \"final_ppl\": \"Final Val PPL\"}\n",
    "    if run_inference_benchmark:\n",
    "        cols_to_rename.update({\"fwd_pass_latency_ms\": \"Fwd Latency (ms)\", \"fwd_pass_throughput\": \"Fwd TP (samples/s)\", \"gen_latency_ms\": \"Gen Latency (ms)\", \"gen_throughput\": \"Gen TP (samples/s)\"})\n",
    "    df_display = df.rename(columns=cols_to_rename)\n",
    "    display_columns_present = [col for col in [\"Final Val PPL\", \"Peak GPU Mem (MB) Eval\", \"Fwd Latency (ms)\", \"Fwd TP (samples/s)\", \"Gen Latency (ms)\", \"Gen TP (samples/s)\"] if col in df_display.columns]\n",
    "    df_display = df_display[display_columns_present]\n",
    "    format_map = {\"Peak GPU Mem (MB) Eval\": '{:,.1f}', \"Final Val PPL\": '{:.2f}', \"Fwd Latency (ms)\": '{:.1f}', \"Fwd TP (samples/s)\": '{:.1f}', \"Gen Latency (ms)\": '{:.1f}', \"Gen TP (samples/s)\": '{:.1f}'}\n",
    "    for col, fmt in format_map.items():\n",
    "        if col in df_display.columns:\n",
    "            try: df_display[col] = df_display[col].apply(lambda x: fmt.format(x) if isinstance(x, (int, float)) and pd.notnull(x) and not (isinstance(x, float) and (math.isnan(x) or math.isinf(x))) else x)\n",
    "            except Exception: logger.warning(f\"Could not format column {col}. Skipping formatting.\")\n",
    "    logger.info(\"\\nComparison DataFrame:\\n%s\", df_display.to_string())\n",
    "    if run:\n",
    "        try: \n",
    "            df_log = df_display.reset_index().rename(columns={'index': 'Method'})\n",
    "            wandb.log({\"Quantization_Impact_Comparison_Table\": wandb.Table(dataframe=df_log)}) # Changed table name\n",
    "            logger.info(\"Comparison table logged to Weights & Biases.\")\n",
    "        except Exception as e: logger.error(f\"Failed to log DataFrame to Weights & Biases: {e}\")\n",
    "else:\n",
    "    logger.error(\"No successful benchmark runs to compare.\")\n",
    "\n",
    "if run:\n",
    "    wandb.finish()\n",
    "    logger.info(\"Weights & Biases run finished.\")\n",
    "logger.info(\"\\n===== Script Finished =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cfb74-d22d-4d7d-be87-7facff5053d7",
   "metadata": {},
   "source": [
    "# Group 2 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd1b5e1-7387-4539-a1df-3a88127f892b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, dataset, batch_size, max_length=128, num_batches=100):\n",
    "    import time, torch, numpy as np\n",
    "    device = next(model.parameters()).device\n",
    "    test_texts = [t for t in dataset[\"test\"][\"text\"] if t.strip()]\n",
    "\n",
    "    model.eval()\n",
    "    infer_times, infer_mems = [], []\n",
    "    infer_thrpts, infer_perps = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            batch = test_texts[i*batch_size:(i+1)*batch_size]\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            loss = outputs.loss\n",
    "            perp = torch.exp(loss).item()\n",
    "            elapsed = time.time() - start\n",
    "            mem = torch.cuda.memory_allocated(device) / 1024**2\n",
    "\n",
    "            infer_times.append(elapsed)\n",
    "            infer_mems.append(mem)\n",
    "            infer_thrpts.append(batch_size / elapsed)\n",
    "            infer_perps.append(perp)\n",
    "\n",
    "    return {\n",
    "        \"time\":       (np.mean(infer_times),   np.std(infer_times)),\n",
    "        \"memory\":     (np.mean(infer_mems),    np.std(infer_mems)),\n",
    "        \"throughput\": (np.mean(infer_thrpts),  np.std(infer_thrpts)),\n",
    "        \"perplexity\": (np.mean(infer_perps),   np.std(infer_perps))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8de31c7-0501-4832-b64a-d9878843a38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 01:44:10,245 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-05-08 01:44:10,718 - INFO - \n",
      "Running benchmark for Quant+Flash model with batch_size=8, max_length=128\n",
      "2025-05-08 01:44:10,719 - INFO - Configuration:\n",
      "2025-05-08 01:44:10,720 - INFO -   • Model:  Quant+Flash on GPT-2\n",
      "2025-05-08 01:44:10,720 - INFO -   • Batch size:   8\n",
      "2025-05-08 01:44:10,721 - INFO -   • Max length:   128\n",
      "2025-05-08 01:44:14,548 - INFO - \n",
      "Inference:\n",
      "2025-05-08 01:44:14,549 - INFO -   Average time per batch:    0.0305 ± 0.0046 seconds\n",
      "2025-05-08 01:44:14,549 - INFO -   Average memory usage:      277.50 ± 37.13 MB\n",
      "2025-05-08 01:44:14,550 - INFO -   Average throughput:        268.01 ± 41.32 samples/second\n",
      "2025-05-08 01:44:14,551 - INFO -   Average perplexity:        11.6864 ± 7.8461\n",
      "2025-05-08 01:44:14,551 - INFO - \n",
      "Running benchmark for Quant+Flash model with batch_size=16, max_length=128\n",
      "2025-05-08 01:44:14,552 - INFO - Configuration:\n",
      "2025-05-08 01:44:14,553 - INFO -   • Model:  Quant+Flash on GPT-2\n",
      "2025-05-08 01:44:14,553 - INFO -   • Batch size:   16\n",
      "2025-05-08 01:44:14,554 - INFO -   • Max length:   128\n",
      "2025-05-08 01:44:21,475 - INFO - \n",
      "Inference:\n",
      "2025-05-08 01:44:21,476 - INFO -   Average time per batch:    0.0557 ± 0.0103 seconds\n",
      "2025-05-08 01:44:21,477 - INFO -   Average memory usage:      420.30 ± 69.48 MB\n",
      "2025-05-08 01:44:21,477 - INFO -   Average throughput:        309.83 ± 122.09 samples/second\n",
      "2025-05-08 01:44:21,478 - INFO -   Average perplexity:        9.7792 ± 5.6974\n",
      "2025-05-08 01:44:21,479 - INFO - \n",
      "Running benchmark for Quant+Flash model with batch_size=32, max_length=128\n",
      "2025-05-08 01:44:21,479 - INFO - Configuration:\n",
      "2025-05-08 01:44:21,480 - INFO -   • Model:  Quant+Flash on GPT-2\n",
      "2025-05-08 01:44:21,481 - INFO -   • Batch size:   32\n",
      "2025-05-08 01:44:21,482 - INFO -   • Max length:   128\n",
      "2025-05-08 01:44:34,249 - INFO - \n",
      "Inference:\n",
      "2025-05-08 01:44:34,250 - INFO -   Average time per batch:    0.1151 ± 0.0174 seconds\n",
      "2025-05-08 01:44:34,251 - INFO -   Average memory usage:      728.05 ± 91.21 MB\n",
      "2025-05-08 01:44:34,252 - INFO -   Average throughput:        299.48 ± 143.62 samples/second\n",
      "2025-05-08 01:44:34,252 - INFO -   Average perplexity:        8.5246 ± 4.1975\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "model_path_to_evaluate = \"./saved_models/distilled_lora_prepruned\"\n",
    "bnb_config_for_4bit_quantized_load = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path_to_evaluate,\n",
    "                quantization_config=bnb_config_for_4bit_quantized_load,\n",
    "                device_map=\"auto\",\n",
    "                config=flash_cfg   \n",
    "            )\n",
    "\n",
    "for bs in [8, 16, 32]:\n",
    "    logger.info(f\"\\nRunning benchmark for Quant+Flash model with batch_size={bs}, max_length={max_length}\")\n",
    "    logger.info(\"Configuration:\")\n",
    "    logger.info(f\"  • Model:  Quant+Flash on GPT-2\")\n",
    "    logger.info(f\"  • Batch size:   {bs}\")\n",
    "    logger.info(f\"  • Max length:   {max_length}\")\n",
    "\n",
    "    infer_stats = benchmark_inference(model, tokenizer, dataset, batch_size=bs)\n",
    "\n",
    "#     # Training results\n",
    "#     t = train_stats\n",
    "#     logger.info(\"\\nTraining:\")\n",
    "#     logger.info(f\"  Average time per batch:    {t['time'][0]:.4f} ± {t['time'][1]:.4f} seconds\")\n",
    "#     logger.info(f\"  Average memory usage:      {t['memory'][0]:.2f} ± {t['memory'][1]:.2f} MB\")\n",
    "#     logger.info(f\"  Average throughput:        {t['throughput'][0]:.2f} ± {t['throughput'][1]:.2f} samples/second\")\n",
    "#     logger.info(f\"  Average loss:              {t['loss'][0]:.4f} ± {t['loss'][1]:.4f}\")\n",
    "#     logger.info(f\"  Average perplexity:        {t['perplexity'][0]:.4f} ± {t['perplexity'][1]:.4f}\")\n",
    "\n",
    "    # Inference results\n",
    "    i = infer_stats\n",
    "    logger.info(\"\\nInference:\")\n",
    "    logger.info(f\"  Average time per batch:    {i['time'][0]:.4f} ± {i['time'][1]:.4f} seconds\")\n",
    "    logger.info(f\"  Average memory usage:      {i['memory'][0]:.2f} ± {i['memory'][1]:.2f} MB\")\n",
    "    logger.info(f\"  Average throughput:        {i['throughput'][0]:.2f} ± {i['throughput'][1]:.2f} samples/second\")\n",
    "    logger.info(f\"  Average perplexity:        {i['perplexity'][0]:.4f} ± {i['perplexity'][1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
