{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93729611-4746-4567-b11d-ee98b7ef5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import platform\n",
    "import time\n",
    "import wandb \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import json, pprint\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bf259-1f56-444d-92fb-ed497ee4ece4",
   "metadata": {},
   "source": [
    "# Wandb Benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b099431b-6b98-4b21-af0b-3b2bdba5cc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.9\n",
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU model: Tesla T4\n",
      "Number of GPUs: 1\n",
      "Available GPU memory: 15.64 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No Cuda!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301e9804-6826-437b-904e-873beeb6cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:36:05,537 - INFO - Using primary device for stats: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tgs2126/wandb/run-20250507_063605-uvgom8ej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hpml_final_project/quantization-impact-comparison/runs/uvgom8ej' target=\"_blank\">Eval_distilled_lora_prepruned_vs_4bit_UserLoad</a></strong> to <a href='https://wandb.ai/hpml_final_project/quantization-impact-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hpml_final_project/quantization-impact-comparison' target=\"_blank\">https://wandb.ai/hpml_final_project/quantization-impact-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hpml_final_project/quantization-impact-comparison/runs/uvgom8ej' target=\"_blank\">https://wandb.ai/hpml_final_project/quantization-impact-comparison/runs/uvgom8ej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:36:06,342 - INFO - Weights & Biases initialized successfully.\n",
      "2025-05-07 06:36:06,343 - INFO - Loading data...\n",
      "2025-05-07 06:36:10,265 - INFO - Using tokenizer: gpt2 with padding_side='right'\n",
      "2025-05-07 06:36:10,273 - INFO - Data loaded. Val/Test texts: 2461\n",
      "2025-05-07 06:36:10,275 - INFO - \n",
      "===== Evaluating: Distilled LoRA Pre-Pruned (Loaded FP) from ./saved_models/distilled_lora_prepruned =====\n",
      "2025-05-07 06:36:10,276 - INFO - Loading model as GPT2LMHeadModel and moving to device: cuda...\n",
      "2025-05-07 06:36:10,543 - INFO - Successfully loaded Distilled LoRA Pre-Pruned (Loaded FP)\n",
      "2025-05-07 06:36:10,544 - INFO - Model type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "2025-05-07 06:36:36,806 - INFO - Distilled LoRA Pre-Pruned (Loaded FP) - Final Validation Perplexity: 12.53\n",
      "2025-05-07 06:36:36,807 - INFO - Running inference benchmark for Distilled LoRA Pre-Pruned (Loaded FP)...\n",
      "2025-05-07 06:36:36,809 - INFO - --- Starting Inference Benchmark (Generation: False) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ddd6ef3e31411187be8cc59aba4725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=False:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:36:40,863 - INFO - --- Finished Inference Benchmark (Generation: False) ---\n",
      "2025-05-07 06:36:40,866 - INFO - --- Starting Inference Benchmark (Generation: True) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b60011362d4198a0d95253de6c6d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=True:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-05-07 06:36:43,839 - INFO - --- Finished Inference Benchmark (Generation: True) ---\n",
      "2025-05-07 06:36:44,083 - INFO - \n",
      "===== Evaluating: Distilled LoRA Pre-Pruned (Loaded Quant 4-bit) from ./saved_models/distilled_lora_prepruned =====\n",
      "2025-05-07 06:36:44,084 - INFO - Loading model with 4-bit quantization and device_map='auto'...\n",
      "2025-05-07 06:36:44,154 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-05-07 06:36:44,590 - INFO - Successfully loaded Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)\n",
      "2025-05-07 06:36:44,592 - INFO - Model type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "2025-05-07 06:36:56,184 - INFO - Distilled LoRA Pre-Pruned (Loaded Quant 4-bit) - Final Validation Perplexity: 12.73\n",
      "2025-05-07 06:36:56,185 - INFO - Running inference benchmark for Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)...\n",
      "2025-05-07 06:36:56,187 - INFO - --- Starting Inference Benchmark (Generation: False) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a451873daf940d997766e5cb107d0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=False:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:36:57,819 - INFO - --- Finished Inference Benchmark (Generation: False) ---\n",
      "2025-05-07 06:36:57,822 - INFO - --- Starting Inference Benchmark (Generation: True) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d03f58801a4ab2b2c125772c9e91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference Gen=True:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "2025-05-07 06:37:00,477 - INFO - --- Finished Inference Benchmark (Generation: True) ---\n",
      "2025-05-07 06:37:00,701 - INFO - \n",
      "===== Final Evaluation Comparison =====\n",
      "2025-05-07 06:37:00,707 - INFO - \n",
      "Comparison DataFrame:\n",
      "                                               Final Val PPL Peak GPU Mem (MB) Eval Fwd Latency (ms) Fwd TP (samples/s) Gen Latency (ms) Gen TP (samples/s)\n",
      "Distilled LoRA Pre-Pruned (Loaded FP)                  12.53                1,305.8              9.3              107.3             13.9               72.0\n",
      "Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)         12.73                  839.0              3.3              299.1             12.3               81.6\n",
      "2025-05-07 06:37:01,089 - INFO - Comparison table logged to Weights & Biases.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/final_ppl</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/peak_mem_mb_eval</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/final_ppl</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_latency_ms</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_throughput</td><td>▁</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/peak_mem_mb_eval</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/final_ppl</td><td>12.53347</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_latency_ms</td><td>9.32335</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/fwd_pass_throughput</td><td>107.2576</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_latency_ms</td><td>13.88996</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/gen_throughput</td><td>71.99445</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded FP)/peak_mem_mb_eval</td><td>1305.7959</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/final_ppl</td><td>12.72799</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_latency_ms</td><td>3.34296</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/fwd_pass_throughput</td><td>299.13624</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_latency_ms</td><td>12.25649</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/gen_throughput</td><td>81.5894</td></tr><tr><td>Summary/Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)/peak_mem_mb_eval</td><td>839.02734</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Eval_distilled_lora_prepruned_vs_4bit_UserLoad</strong> at: <a href='https://wandb.ai/hpml_final_project/quantization-impact-comparison/runs/uvgom8ej' target=\"_blank\">https://wandb.ai/hpml_final_project/quantization-impact-comparison/runs/uvgom8ej</a><br> View project at: <a href='https://wandb.ai/hpml_final_project/quantization-impact-comparison' target=\"_blank\">https://wandb.ai/hpml_final_project/quantization-impact-comparison</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250507_063605-uvgom8ej/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 06:37:03,288 - INFO - Weights & Biases run finished.\n",
      "2025-05-07 06:37:03,289 - INFO - \n",
      "===== Script Finished =====\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time, math, logging\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import gc\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json # Not strictly needed in this version but often useful with adapters\n",
    "# from peft import PeftModel # Not needed if we load merged models only\n",
    "\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", force=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using primary device for stats: {device}\")\n",
    "\n",
    "model_path_to_evaluate = \"./saved_models/distilled_lora_prepruned\" # Path to your saved merged model\n",
    "original_base_model_name_for_tokenizer = \"gpt2\" # Tokenizer should ideally match what was saved with merged_fp16\n",
    "max_length = 128\n",
    "inference_batch_size = 8\n",
    "num_inference_batches = 50\n",
    "run_inference_benchmark = True\n",
    "\n",
    "bnb_config_for_4bit_quantized_load = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        project=\"quantization-impact-comparison\", # Updated project\n",
    "        name=f\"Eval_{os.path.basename(model_path_to_evaluate)}_vs_4bit_UserLoad\",\n",
    "        config={\n",
    "            \"model_path_evaluated\": model_path_to_evaluate,\n",
    "            \"original_base_model_name_for_tokenizer\": original_base_model_name_for_tokenizer,\n",
    "            \"max_length\": max_length,\n",
    "            \"inference_batch_size\": inference_batch_size,\n",
    "            \"num_inference_batches\": num_inference_batches,\n",
    "            \"run_inference_benchmark\": run_inference_benchmark,\n",
    "            \"bnb_config_for_4bit_version\": bnb_config_for_4bit_quantized_load.to_dict()\n",
    "        }\n",
    "    )\n",
    "    logger.info(\"Weights & Biases initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Weights & Biases: {e}\")\n",
    "    run = None\n",
    "\n",
    "logger.info(\"Loading data...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(original_base_model_name_for_tokenizer)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "logger.info(f\"Using tokenizer: {tokenizer.name_or_path} with padding_side='{tokenizer.padding_side}'\")\n",
    "\n",
    "val_texts_full = [t for t in dataset[\"validation\"][\"text\"] if t.strip()]\n",
    "test_texts_full = val_texts_full[:inference_batch_size * num_inference_batches]\n",
    "logger.info(f\"Data loaded. Val/Test texts: {len(val_texts_full)}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model, tokenizer, texts, device, batch_size=8, max_length=128):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    total_evaluated = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        if not batch: continue\n",
    "        total_evaluated += len(batch)\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs, labels=inputs.input_ids)\n",
    "        if hasattr(outputs, 'loss') and outputs.loss is not None:\n",
    "            losses.append(outputs.loss.item() * len(batch))\n",
    "    if not losses or total_evaluated == 0: return float('inf')\n",
    "    avg_loss = sum(losses) / total_evaluated\n",
    "    if avg_loss <= 0: return float('inf')\n",
    "    return math.exp(avg_loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_inference(model, tokenizer, texts, eval_device_ignored, batch_size=8, max_length=128, num_batches=50, generation=False):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    total_samples = 0\n",
    "    logger.info(f\"--- Starting Inference Benchmark (Generation: {generation}) ---\")\n",
    "    generation_config = GenerationConfig(max_new_tokens=5, pad_token_id=tokenizer.pad_token_id, eos_token_id=model.config.eos_token_id if hasattr(model.config, 'eos_token_id') else tokenizer.eos_token_id) if generation else None\n",
    "    \n",
    "    for i in tqdm(range(0, min(len(texts), batch_size * num_batches), batch_size), desc=f\"Inference Gen={generation}\", leave=False):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        if not batch_texts: continue\n",
    "        \n",
    "        model_input_device = next(model.parameters()).device\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(model_input_device)\n",
    "        \n",
    "        batch_samples = inputs['input_ids'].shape[0]; total_samples += batch_samples\n",
    "        start_time = time.perf_counter()\n",
    "        if generation: _ = model.generate(**inputs, generation_config=generation_config)\n",
    "        else: _ = model(**inputs)\n",
    "        if model_input_device.type == \"cuda\": torch.cuda.synchronize(model_input_device)\n",
    "        end_time = time.perf_counter(); latencies.append(end_time - start_time)\n",
    "    if not latencies: return {\"avg_inference_latency_ms_per_sample\": float('nan'), \"avg_inference_throughput_samples_sec\": float('nan')}\n",
    "    total_time_secs = sum(latencies)\n",
    "    throughput_samples_sec = total_samples / total_time_secs if total_time_secs > 0 else 0\n",
    "    avg_latency_sample = (total_time_secs / total_samples) * 1000 if total_samples > 0 else 0\n",
    "    logger.info(f\"--- Finished Inference Benchmark (Generation: {generation}) ---\")\n",
    "    return {\"avg_inference_latency_ms_per_sample\": avg_latency_sample, \"avg_inference_throughput_samples_sec\": throughput_samples_sec}\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "def evaluate_model_version(model_load_path, model_label, load_quantized, bnb_config=None):\n",
    "    logger.info(f\"\\n===== Evaluating: {model_label} from {model_load_path} =====\")\n",
    "    model = None\n",
    "    eval_results = {}\n",
    "    if device.type == \"cuda\": torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(model_load_path):\n",
    "            raise FileNotFoundError(f\"Model path does not exist: {model_load_path}\")\n",
    "\n",
    "        if load_quantized and bnb_config:\n",
    "            logger.info(\"Loading model with 4-bit quantization and device_map='auto'...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_load_path,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"Loading model as GPT2LMHeadModel and moving to device: {device}...\")\n",
    "            model = GPT2LMHeadModel.from_pretrained(model_load_path).to(device)\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {model_label}\")\n",
    "        logger.info(f\"Model type: {type(model)}\")\n",
    "        \n",
    "        eval_device_for_model = next(model.parameters()).device if next(model.parameters(), None) is not None else device\n",
    "\n",
    "        ppl = compute_perplexity(model, tokenizer, val_texts_full, eval_device_for_model,\n",
    "                                   batch_size=inference_batch_size, max_length=max_length)\n",
    "        eval_results[\"final_ppl\"] = ppl\n",
    "        logger.info(f\"{model_label} - Final Validation Perplexity: {ppl:.2f}\")\n",
    "\n",
    "        if run_inference_benchmark:\n",
    "            logger.info(f\"Running inference benchmark for {model_label}...\")\n",
    "            inference_res_fwd = benchmark_inference(model, tokenizer, test_texts_full, eval_device_for_model, batch_size=inference_batch_size, max_length=max_length, num_batches=num_inference_batches, generation=False)\n",
    "            inference_res_gen = benchmark_inference(model, tokenizer, test_texts_full, eval_device_for_model, batch_size=inference_batch_size, max_length=max_length, num_batches=num_inference_batches // 2, generation=True)\n",
    "            eval_results.update({\n",
    "                 \"fwd_pass_latency_ms\": inference_res_fwd[\"avg_inference_latency_ms_per_sample\"],\n",
    "                 \"fwd_pass_throughput\": inference_res_fwd[\"avg_inference_throughput_samples_sec\"],\n",
    "                 \"gen_latency_ms\": inference_res_gen[\"avg_inference_latency_ms_per_sample\"],\n",
    "                 \"gen_throughput\": inference_res_gen[\"avg_inference_throughput_samples_sec\"]})\n",
    "        \n",
    "        current_mem_eval = 0\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize(); current_mem_eval = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "        eval_results[\"peak_mem_mb_eval\"] = current_mem_eval\n",
    "        \n",
    "        all_results[model_label] = eval_results\n",
    "        if run and 'final_ppl' in eval_results and not math.isinf(eval_results['final_ppl']):\n",
    "            wandb.log({f\"Summary/{model_label}/{k}\": v for k, v in eval_results.items()})\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during {model_label} evaluation: {e}\")\n",
    "        all_results[model_label] = {\"final_ppl\": float('inf'), \"peak_mem_mb_eval\": \"Error\"}\n",
    "    finally:\n",
    "        if model is not None: del model\n",
    "        gc.collect(); torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "\n",
    "evaluate_model_version(\n",
    "    model_load_path=model_path_to_evaluate,\n",
    "    model_label=\"Distilled LoRA Pre-Pruned (Loaded FP)\", \n",
    "    load_quantized=False\n",
    ")\n",
    "\n",
    "evaluate_model_version(\n",
    "    model_load_path=model_path_to_evaluate, \n",
    "    model_label=\"Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)\",\n",
    "    load_quantized=True,\n",
    "    bnb_config=bnb_config_for_4bit_quantized_load\n",
    ")\n",
    "\n",
    "logger.info(\"\\n===== Final Evaluation Comparison =====\")\n",
    "results_list_df = []\n",
    "indices_df = []\n",
    "ordered_labels = [\n",
    "    \"Distilled LoRA Pre-Pruned (Loaded FP)\",\n",
    "    \"Distilled LoRA Pre-Pruned (Loaded Quant 4-bit)\"\n",
    "]\n",
    "for label in ordered_labels:\n",
    "    if label in all_results:\n",
    "        results_list_df.append(all_results[label])\n",
    "        indices_df.append(label)\n",
    "\n",
    "if results_list_df:\n",
    "    df = pd.DataFrame(results_list_df, index=indices_df)\n",
    "    cols_to_rename = {\"peak_mem_mb_eval\": \"Peak GPU Mem (MB) Eval\", \"final_ppl\": \"Final Val PPL\"}\n",
    "    if run_inference_benchmark:\n",
    "        cols_to_rename.update({\"fwd_pass_latency_ms\": \"Fwd Latency (ms)\", \"fwd_pass_throughput\": \"Fwd TP (samples/s)\", \"gen_latency_ms\": \"Gen Latency (ms)\", \"gen_throughput\": \"Gen TP (samples/s)\"})\n",
    "    df_display = df.rename(columns=cols_to_rename)\n",
    "    display_columns_present = [col for col in [\"Final Val PPL\", \"Peak GPU Mem (MB) Eval\", \"Fwd Latency (ms)\", \"Fwd TP (samples/s)\", \"Gen Latency (ms)\", \"Gen TP (samples/s)\"] if col in df_display.columns]\n",
    "    df_display = df_display[display_columns_present]\n",
    "    format_map = {\"Peak GPU Mem (MB) Eval\": '{:,.1f}', \"Final Val PPL\": '{:.2f}', \"Fwd Latency (ms)\": '{:.1f}', \"Fwd TP (samples/s)\": '{:.1f}', \"Gen Latency (ms)\": '{:.1f}', \"Gen TP (samples/s)\": '{:.1f}'}\n",
    "    for col, fmt in format_map.items():\n",
    "        if col in df_display.columns:\n",
    "            try: df_display[col] = df_display[col].apply(lambda x: fmt.format(x) if isinstance(x, (int, float)) and pd.notnull(x) and not (isinstance(x, float) and (math.isnan(x) or math.isinf(x))) else x)\n",
    "            except Exception: logger.warning(f\"Could not format column {col}. Skipping formatting.\")\n",
    "    logger.info(\"\\nComparison DataFrame:\\n%s\", df_display.to_string())\n",
    "    if run:\n",
    "        try: \n",
    "            df_log = df_display.reset_index().rename(columns={'index': 'Method'})\n",
    "            wandb.log({\"Quantization_Impact_Comparison_Table\": wandb.Table(dataframe=df_log)}) # Changed table name\n",
    "            logger.info(\"Comparison table logged to Weights & Biases.\")\n",
    "        except Exception as e: logger.error(f\"Failed to log DataFrame to Weights & Biases: {e}\")\n",
    "else:\n",
    "    logger.error(\"No successful benchmark runs to compare.\")\n",
    "\n",
    "if run:\n",
    "    wandb.finish()\n",
    "    logger.info(\"Weights & Biases run finished.\")\n",
    "logger.info(\"\\n===== Script Finished =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1b5e1-7387-4539-a1df-3a88127f892b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de31c7-0501-4832-b64a-d9878843a38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40f285-db42-4353-8ac2-09635131d628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f9e2d-a1f9-41f4-ab12-54d1c12082c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf840d-5a58-41bb-ba88-396505aaac0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d882bedb-802e-4051-bfcf-60e863d02b71",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4df94f-f1b8-4d4f-b036-fea783e2df11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (benim)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
