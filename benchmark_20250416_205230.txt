2025-04-16 20:52:30,168 - INFO - Python version: 3.10.15
2025-04-16 20:52:30,169 - INFO - PyTorch version: 2.4.1+cu121
2025-04-16 20:52:30,169 - INFO - CUDA available: True
2025-04-16 20:52:30,169 - INFO - CUDA version: 12.1
2025-04-16 20:52:30,169 - INFO - GPU model: Tesla T4
2025-04-16 20:52:30,169 - INFO - Number of GPUs: 1
2025-04-16 20:52:30,169 - INFO - Available GPU memory: 15.64 GB
2025-04-16 20:52:30,170 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=False, use_quantization=False, batch_size=8, max_length=128)
2025-04-16 20:52:30,171 - INFO - 
Model Configuration:
2025-04-16 20:52:30,171 - INFO - Model name: gpt2
2025-04-16 20:52:30,171 - INFO - Use Flash Attention: False
2025-04-16 20:52:30,171 - INFO - Use Quantization: False
2025-04-16 20:52:30,171 - INFO - Batch size: 8
2025-04-16 20:52:30,171 - INFO - Max length: 128
2025-04-16 20:52:30,171 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:52:31,304 - INFO - Loading dataset...
2025-04-16 20:52:36,543 - INFO - Starting comprehensive benchmark...
2025-04-16 20:52:38,150 - INFO - Starting training benchmark...
2025-04-16 20:52:59,491 - INFO - Starting inference benchmark...
2025-04-16 20:53:06,896 - INFO - 
Benchmark Results:
2025-04-16 20:53:06,897 - INFO - Training:
2025-04-16 20:53:06,897 - INFO - Average time per batch: 0.0325 ± 0.0479 seconds
2025-04-16 20:53:06,897 - INFO - Average memory usage: 1256.24 ± 79.81 MB
2025-04-16 20:53:06,898 - INFO - Average throughput: 287.89 ± 35.38 samples/second
2025-04-16 20:53:06,898 - INFO - Average loss: 6.6978 ± 0.9595
2025-04-16 20:53:06,898 - INFO - Average perplexity: 1307.4904 ± 1486.0060
2025-04-16 20:53:06,898 - INFO - 
Inference:
2025-04-16 20:53:06,899 - INFO - Average time per batch: 0.0110 ± 0.0012 seconds
2025-04-16 20:53:06,899 - INFO - Average memory usage: 780.44 ± 73.07 MB
2025-04-16 20:53:06,899 - INFO - Average throughput: 734.54 ± 74.54 samples/second
2025-04-16 20:53:06,899 - INFO - Average perplexity: 1971.5901 ± 3014.9935
2025-04-16 20:53:08,092 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=False, use_quantization=True, batch_size=8, max_length=128)
2025-04-16 20:53:08,092 - INFO - 
Model Configuration:
2025-04-16 20:53:08,092 - INFO - Model name: gpt2
2025-04-16 20:53:08,092 - INFO - Use Flash Attention: False
2025-04-16 20:53:08,092 - INFO - Use Quantization: True
2025-04-16 20:53:08,092 - INFO - Batch size: 8
2025-04-16 20:53:08,093 - INFO - Max length: 128
2025-04-16 20:53:08,093 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:53:08,847 - INFO - Applying quantization to model...
2025-04-16 20:53:08,853 - INFO - Model converted to FP16
2025-04-16 20:53:08,854 - INFO - Loading dataset...
2025-04-16 20:53:13,076 - INFO - Starting comprehensive benchmark...
2025-04-16 20:53:13,913 - INFO - Starting training benchmark...
2025-04-16 20:53:21,511 - INFO - Starting inference benchmark...
2025-04-16 20:53:24,727 - INFO - 
Benchmark Results:
2025-04-16 20:53:24,728 - INFO - Training:
2025-04-16 20:53:24,728 - INFO - Average time per batch: 0.0283 ± 0.0207 seconds
2025-04-16 20:53:24,728 - INFO - Average memory usage: 658.08 ± 40.46 MB
2025-04-16 20:53:24,729 - INFO - Average throughput: 309.32 ± 42.73 samples/second
2025-04-16 20:53:24,729 - INFO - Average loss: 6.6882 ± 0.9648
2025-04-16 20:53:24,729 - INFO - Average perplexity: 1307.4742 ± 1521.9789
2025-04-16 20:53:24,729 - INFO - 
Inference:
2025-04-16 20:53:24,729 - INFO - Average time per batch: 0.0106 ± 0.0014 seconds
2025-04-16 20:53:24,730 - INFO - Average memory usage: 417.49 ± 36.93 MB
2025-04-16 20:53:24,730 - INFO - Average throughput: 764.40 ± 93.50 samples/second
2025-04-16 20:53:24,730 - INFO - Average perplexity: 1967.9449 ± 3008.9316
2025-04-16 20:53:25,697 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=True, use_quantization=False, batch_size=8, max_length=128)
2025-04-16 20:53:25,697 - INFO - 
Model Configuration:
2025-04-16 20:53:25,698 - INFO - Model name: gpt2
2025-04-16 20:53:25,698 - INFO - Use Flash Attention: True
2025-04-16 20:53:25,698 - INFO - Use Quantization: False
2025-04-16 20:53:25,698 - INFO - Batch size: 8
2025-04-16 20:53:25,698 - INFO - Max length: 128
2025-04-16 20:53:25,698 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:53:26,089 - INFO - Flash Attention enabled in model config
2025-04-16 20:53:26,461 - INFO - Loading dataset...
2025-04-16 20:53:30,442 - INFO - Starting comprehensive benchmark...
2025-04-16 20:53:31,312 - INFO - Starting training benchmark...
2025-04-16 20:53:53,281 - INFO - Starting inference benchmark...
2025-04-16 20:54:00,971 - INFO - 
Benchmark Results:
2025-04-16 20:54:00,971 - INFO - Training:
2025-04-16 20:54:00,972 - INFO - Average time per batch: 0.0280 ± 0.0021 seconds
2025-04-16 20:54:00,972 - INFO - Average memory usage: 1252.81 ± 79.82 MB
2025-04-16 20:54:00,972 - INFO - Average throughput: 286.71 ± 20.53 samples/second
2025-04-16 20:54:00,972 - INFO - Average loss: 6.6800 ± 0.9662
2025-04-16 20:54:00,973 - INFO - Average perplexity: 1296.0266 ± 1497.2316
2025-04-16 20:54:00,973 - INFO - 
Inference:
2025-04-16 20:54:00,973 - INFO - Average time per batch: 0.0105 ± 0.0008 seconds
2025-04-16 20:54:00,973 - INFO - Average memory usage: 780.34 ± 73.55 MB
2025-04-16 20:54:00,974 - INFO - Average throughput: 765.14 ± 46.92 samples/second
2025-04-16 20:54:00,974 - INFO - Average perplexity: 1971.5901 ± 3014.9935
2025-04-16 20:54:02,514 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=True, use_quantization=True, batch_size=8, max_length=128)
2025-04-16 20:54:02,514 - INFO - 
Model Configuration:
2025-04-16 20:54:02,514 - INFO - Model name: gpt2
2025-04-16 20:54:02,514 - INFO - Use Flash Attention: True
2025-04-16 20:54:02,514 - INFO - Use Quantization: True
2025-04-16 20:54:02,514 - INFO - Batch size: 8
2025-04-16 20:54:02,514 - INFO - Max length: 128
2025-04-16 20:54:02,514 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:54:02,908 - INFO - Flash Attention enabled in model config
2025-04-16 20:54:03,490 - INFO - Applying quantization to model...
2025-04-16 20:54:03,494 - INFO - Model converted to FP16
2025-04-16 20:54:03,494 - INFO - Loading dataset...
2025-04-16 20:54:06,994 - INFO - Starting comprehensive benchmark...
2025-04-16 20:54:08,044 - INFO - Starting training benchmark...
2025-04-16 20:54:15,596 - INFO - Starting inference benchmark...
2025-04-16 20:54:18,850 - INFO - 
Benchmark Results:
2025-04-16 20:54:18,851 - INFO - Training:
2025-04-16 20:54:18,851 - INFO - Average time per batch: 0.0253 ± 0.0019 seconds
2025-04-16 20:54:18,851 - INFO - Average memory usage: 668.57 ± 40.72 MB
2025-04-16 20:54:18,851 - INFO - Average throughput: 317.64 ± 22.38 samples/second
2025-04-16 20:54:18,851 - INFO - Average loss: 6.6946 ± 0.9826
2025-04-16 20:54:18,852 - INFO - Average perplexity: 1346.2272 ± 1646.2914
2025-04-16 20:54:18,852 - INFO - 
Inference:
2025-04-16 20:54:18,852 - INFO - Average time per batch: 0.0101 ± 0.0010 seconds
2025-04-16 20:54:18,852 - INFO - Average memory usage: 427.81 ± 37.00 MB
2025-04-16 20:54:18,853 - INFO - Average throughput: 800.11 ± 69.29 samples/second
2025-04-16 20:54:18,853 - INFO - Average perplexity: 1967.9449 ± 3008.9316
2025-04-16 20:54:19,871 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=False, use_quantization=False, batch_size=16, max_length=128)
2025-04-16 20:54:19,871 - INFO - 
Model Configuration:
2025-04-16 20:54:19,871 - INFO - Model name: gpt2
2025-04-16 20:54:19,872 - INFO - Use Flash Attention: False
2025-04-16 20:54:19,872 - INFO - Use Quantization: False
2025-04-16 20:54:19,872 - INFO - Batch size: 16
2025-04-16 20:54:19,872 - INFO - Max length: 128
2025-04-16 20:54:19,872 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:54:20,621 - INFO - Loading dataset...
2025-04-16 20:54:23,981 - INFO - Starting comprehensive benchmark...
2025-04-16 20:54:24,985 - INFO - Starting training benchmark...
2025-04-16 20:55:14,314 - INFO - Starting inference benchmark...
2025-04-16 20:55:30,460 - INFO - 
Benchmark Results:
2025-04-16 20:55:30,460 - INFO - Training:
2025-04-16 20:55:30,461 - INFO - Average time per batch: 0.0290 ± 0.0027 seconds
2025-04-16 20:55:30,461 - INFO - Average memory usage: 1576.96 ± 97.77 MB
2025-04-16 20:55:30,461 - INFO - Average throughput: 556.10 ± 46.44 samples/second
2025-04-16 20:55:30,461 - INFO - Average loss: 6.5381 ± 0.8943
2025-04-16 20:55:30,462 - INFO - Average perplexity: 1132.7158 ± 1560.7454
2025-04-16 20:55:30,462 - INFO - 
Inference:
2025-04-16 20:55:30,462 - INFO - Average time per batch: 0.0109 ± 0.0011 seconds
2025-04-16 20:55:30,462 - INFO - Average memory usage: 1067.73 ± 138.30 MB
2025-04-16 20:55:30,463 - INFO - Average throughput: 1482.39 ± 129.89 samples/second
2025-04-16 20:55:30,463 - INFO - Average perplexity: 3126.3715 ± 6117.7732
2025-04-16 20:56:34,069 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=False, use_quantization=True, batch_size=16, max_length=128)
2025-04-16 20:56:34,070 - INFO - 
Model Configuration:
2025-04-16 20:56:34,070 - INFO - Model name: gpt2
2025-04-16 20:56:34,070 - INFO - Use Flash Attention: False
2025-04-16 20:56:34,070 - INFO - Use Quantization: True
2025-04-16 20:56:34,070 - INFO - Batch size: 16
2025-04-16 20:56:34,070 - INFO - Max length: 128
2025-04-16 20:56:34,070 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:56:34,835 - INFO - Applying quantization to model...
2025-04-16 20:56:34,839 - INFO - Model converted to FP16
2025-04-16 20:56:34,839 - INFO - Loading dataset...
2025-04-16 20:56:38,282 - INFO - Starting comprehensive benchmark...
2025-04-16 20:56:39,166 - INFO - Starting training benchmark...
2025-04-16 20:56:54,383 - INFO - Starting inference benchmark...
2025-04-16 20:57:00,716 - INFO - 
Benchmark Results:
2025-04-16 20:57:00,716 - INFO - Training:
2025-04-16 20:57:00,717 - INFO - Average time per batch: 0.0277 ± 0.0025 seconds
2025-04-16 20:57:00,717 - INFO - Average memory usage: 833.98 ± 49.42 MB
2025-04-16 20:57:00,717 - INFO - Average throughput: 581.70 ± 47.36 samples/second
2025-04-16 20:57:00,717 - INFO - Average loss: 6.5650 ± 0.9102
2025-04-16 20:57:00,717 - INFO - Average perplexity: 1218.9453 ± 1943.2884
2025-04-16 20:57:00,718 - INFO - 
Inference:
2025-04-16 20:57:00,718 - INFO - Average time per batch: 0.0109 ± 0.0012 seconds
2025-04-16 20:57:00,718 - INFO - Average memory usage: 572.22 ± 68.53 MB
2025-04-16 20:57:00,718 - INFO - Average throughput: 1486.61 ± 141.64 samples/second
2025-04-16 20:57:00,719 - INFO - Average perplexity: 3118.7186 ± 6101.1234
2025-04-16 20:57:01,818 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=True, use_quantization=False, batch_size=16, max_length=128)
2025-04-16 20:57:01,818 - INFO - 
Model Configuration:
2025-04-16 20:57:01,818 - INFO - Model name: gpt2
2025-04-16 20:57:01,818 - INFO - Use Flash Attention: True
2025-04-16 20:57:01,818 - INFO - Use Quantization: False
2025-04-16 20:57:01,818 - INFO - Batch size: 16
2025-04-16 20:57:01,819 - INFO - Max length: 128
2025-04-16 20:57:01,819 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:57:02,184 - INFO - Flash Attention enabled in model config
2025-04-16 20:57:02,562 - INFO - Loading dataset...
2025-04-16 20:57:05,758 - INFO - Starting comprehensive benchmark...
2025-04-16 20:57:06,663 - INFO - Starting training benchmark...
2025-04-16 20:57:56,203 - INFO - Starting inference benchmark...
2025-04-16 20:58:12,163 - INFO - 
Benchmark Results:
2025-04-16 20:58:12,164 - INFO - Training:
2025-04-16 20:58:12,164 - INFO - Average time per batch: 0.0290 ± 0.0021 seconds
2025-04-16 20:58:12,164 - INFO - Average memory usage: 1576.96 ± 97.77 MB
2025-04-16 20:58:12,165 - INFO - Average throughput: 554.67 ± 40.19 samples/second
2025-04-16 20:58:12,165 - INFO - Average loss: 6.5493 ± 0.9030
2025-04-16 20:58:12,165 - INFO - Average perplexity: 1145.6933 ± 1512.6905
2025-04-16 20:58:12,165 - INFO - 
Inference:
2025-04-16 20:58:12,165 - INFO - Average time per batch: 0.0111 ± 0.0012 seconds
2025-04-16 20:58:12,166 - INFO - Average memory usage: 1067.73 ± 138.30 MB
2025-04-16 20:58:12,166 - INFO - Average throughput: 1451.62 ± 136.09 samples/second
2025-04-16 20:58:12,166 - INFO - Average perplexity: 3126.3715 ± 6117.7732
2025-04-16 20:58:13,325 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=True, use_quantization=True, batch_size=16, max_length=128)
2025-04-16 20:58:13,325 - INFO - 
Model Configuration:
2025-04-16 20:58:13,325 - INFO - Model name: gpt2
2025-04-16 20:58:13,325 - INFO - Use Flash Attention: True
2025-04-16 20:58:13,325 - INFO - Use Quantization: True
2025-04-16 20:58:13,325 - INFO - Batch size: 16
2025-04-16 20:58:13,325 - INFO - Max length: 128
2025-04-16 20:58:13,325 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:58:13,722 - INFO - Flash Attention enabled in model config
2025-04-16 20:58:14,097 - INFO - Applying quantization to model...
2025-04-16 20:58:14,102 - INFO - Model converted to FP16
2025-04-16 20:58:14,102 - INFO - Loading dataset...
2025-04-16 20:58:17,289 - INFO - Starting comprehensive benchmark...
2025-04-16 20:58:18,172 - INFO - Starting training benchmark...
2025-04-16 20:58:33,432 - INFO - Starting inference benchmark...
2025-04-16 20:58:39,714 - INFO - 
Benchmark Results:
2025-04-16 20:58:39,714 - INFO - Training:
2025-04-16 20:58:39,714 - INFO - Average time per batch: 0.0271 ± 0.0020 seconds
2025-04-16 20:58:39,715 - INFO - Average memory usage: 833.98 ± 49.42 MB
2025-04-16 20:58:39,715 - INFO - Average throughput: 593.14 ± 43.12 samples/second
2025-04-16 20:58:39,715 - INFO - Average loss: 6.5381 ± 0.9018
2025-04-16 20:58:39,715 - INFO - Average perplexity: 1135.0634 ± 1520.5649
2025-04-16 20:58:39,715 - INFO - 
Inference:
2025-04-16 20:58:39,716 - INFO - Average time per batch: 0.0102 ± 0.0009 seconds
2025-04-16 20:58:39,716 - INFO - Average memory usage: 572.22 ± 68.53 MB
2025-04-16 20:58:39,716 - INFO - Average throughput: 1581.96 ± 117.38 samples/second
2025-04-16 20:58:39,716 - INFO - Average perplexity: 3118.7186 ± 6101.1234
2025-04-16 20:58:40,663 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=False, use_quantization=False, batch_size=32, max_length=128)
2025-04-16 20:58:40,663 - INFO - 
Model Configuration:
2025-04-16 20:58:40,663 - INFO - Model name: gpt2
2025-04-16 20:58:40,663 - INFO - Use Flash Attention: False
2025-04-16 20:58:40,664 - INFO - Use Quantization: False
2025-04-16 20:58:40,664 - INFO - Batch size: 32
2025-04-16 20:58:40,664 - INFO - Max length: 128
2025-04-16 20:58:40,664 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 20:58:41,390 - INFO - Loading dataset...
2025-04-16 20:58:44,559 - INFO - Starting comprehensive benchmark...
2025-04-16 20:58:45,639 - INFO - Starting training benchmark...
2025-04-16 21:00:29,236 - INFO - Starting inference benchmark...
2025-04-16 21:00:59,733 - INFO - 
Benchmark Results:
2025-04-16 21:00:59,733 - INFO - Training:
2025-04-16 21:00:59,733 - INFO - Average time per batch: 0.0399 ± 0.1073 seconds
2025-04-16 21:00:59,734 - INFO - Average memory usage: 2189.42 ± 121.90 MB
2025-04-16 21:00:59,734 - INFO - Average throughput: 1092.25 ± 131.09 samples/second
2025-04-16 21:00:59,734 - INFO - Average loss: 6.7550 ± 0.8932
2025-04-16 21:00:59,734 - INFO - Average perplexity: 1433.7963 ± 2233.1944
2025-04-16 21:00:59,735 - INFO - 
Inference:
2025-04-16 21:00:59,735 - INFO - Average time per batch: 0.0114 ± 0.0012 seconds
2025-04-16 21:00:59,735 - INFO - Average memory usage: 1683.74 ± 182.86 MB
2025-04-16 21:00:59,735 - INFO - Average throughput: 2836.24 ± 289.02 samples/second
2025-04-16 21:00:59,735 - INFO - Average perplexity: 3173.5368 ± 6510.7586
2025-04-16 21:01:01,542 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=False, use_quantization=True, batch_size=32, max_length=128)
2025-04-16 21:01:01,542 - INFO - 
Model Configuration:
2025-04-16 21:01:01,542 - INFO - Model name: gpt2
2025-04-16 21:01:01,542 - INFO - Use Flash Attention: False
2025-04-16 21:01:01,542 - INFO - Use Quantization: True
2025-04-16 21:01:01,542 - INFO - Batch size: 32
2025-04-16 21:01:01,543 - INFO - Max length: 128
2025-04-16 21:01:01,543 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 21:01:02,308 - INFO - Applying quantization to model...
2025-04-16 21:01:02,312 - INFO - Model converted to FP16
2025-04-16 21:01:02,312 - INFO - Loading dataset...
2025-04-16 21:01:06,435 - INFO - Starting comprehensive benchmark...
2025-04-16 21:01:07,308 - INFO - Starting training benchmark...
2025-04-16 21:01:37,292 - INFO - Starting inference benchmark...
2025-04-16 21:01:49,141 - INFO - 
Benchmark Results:
2025-04-16 21:01:49,141 - INFO - Training:
2025-04-16 21:01:49,142 - INFO - Average time per batch: 0.0270 ± 0.0021 seconds
2025-04-16 21:01:49,142 - INFO - Average memory usage: 1133.32 ± 60.67 MB
2025-04-16 21:01:49,142 - INFO - Average throughput: 1192.02 ± 90.28 samples/second
2025-04-16 21:01:49,142 - INFO - Average loss: 6.7540 ± 0.8869
2025-04-16 21:01:49,142 - INFO - Average perplexity: 1397.4602 ± 2006.1589
2025-04-16 21:01:49,143 - INFO - 
Inference:
2025-04-16 21:01:49,143 - INFO - Average time per batch: 0.0105 ± 0.0012 seconds
2025-04-16 21:01:49,143 - INFO - Average memory usage: 879.74 ± 91.12 MB
2025-04-16 21:01:49,143 - INFO - Average throughput: 3089.21 ± 307.99 samples/second
2025-04-16 21:01:49,144 - INFO - Average perplexity: 3164.9513 ± 6489.3211
2025-04-16 21:01:50,794 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=True, use_quantization=False, batch_size=32, max_length=128)
2025-04-16 21:01:50,794 - INFO - 
Model Configuration:
2025-04-16 21:01:50,795 - INFO - Model name: gpt2
2025-04-16 21:01:50,795 - INFO - Use Flash Attention: True
2025-04-16 21:01:50,795 - INFO - Use Quantization: False
2025-04-16 21:01:50,795 - INFO - Batch size: 32
2025-04-16 21:01:50,795 - INFO - Max length: 128
2025-04-16 21:01:50,795 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 21:01:51,165 - INFO - Flash Attention enabled in model config
2025-04-16 21:01:51,539 - INFO - Loading dataset...
2025-04-16 21:01:54,822 - INFO - Starting comprehensive benchmark...
2025-04-16 21:01:55,669 - INFO - Starting training benchmark...
2025-04-16 21:03:38,845 - INFO - Starting inference benchmark...
2025-04-16 21:04:09,330 - INFO - 
Benchmark Results:
2025-04-16 21:04:09,330 - INFO - Training:
2025-04-16 21:04:09,330 - INFO - Average time per batch: 0.0275 ± 0.0019 seconds
2025-04-16 21:04:09,330 - INFO - Average memory usage: 2189.37 ± 121.94 MB
2025-04-16 21:04:09,331 - INFO - Average throughput: 1169.20 ± 76.89 samples/second
2025-04-16 21:04:09,331 - INFO - Average loss: 6.7529 ± 0.8921
2025-04-16 21:04:09,331 - INFO - Average perplexity: 1424.3680 ± 2156.7037
2025-04-16 21:04:09,331 - INFO - 
Inference:
2025-04-16 21:04:09,331 - INFO - Average time per batch: 0.0107 ± 0.0010 seconds
2025-04-16 21:04:09,332 - INFO - Average memory usage: 1683.74 ± 182.86 MB
2025-04-16 21:04:09,332 - INFO - Average throughput: 3011.75 ± 250.70 samples/second
2025-04-16 21:04:09,332 - INFO - Average perplexity: 3173.5368 ± 6510.7586
2025-04-16 21:04:11,297 - INFO - 
Running benchmark for config: ModelConfig(model_name='gpt2', use_flash_attention=True, use_quantization=True, batch_size=32, max_length=128)
2025-04-16 21:04:11,297 - INFO - 
Model Configuration:
2025-04-16 21:04:11,297 - INFO - Model name: gpt2
2025-04-16 21:04:11,297 - INFO - Use Flash Attention: True
2025-04-16 21:04:11,297 - INFO - Use Quantization: True
2025-04-16 21:04:11,297 - INFO - Batch size: 32
2025-04-16 21:04:11,297 - INFO - Max length: 128
2025-04-16 21:04:11,297 - INFO - 
Loading gpt2 model and tokenizer...
2025-04-16 21:04:11,658 - INFO - Flash Attention enabled in model config
2025-04-16 21:04:12,039 - INFO - Applying quantization to model...
2025-04-16 21:04:12,043 - INFO - Model converted to FP16
2025-04-16 21:04:12,043 - INFO - Loading dataset...
2025-04-16 21:04:15,219 - INFO - Starting comprehensive benchmark...
2025-04-16 21:04:16,160 - INFO - Starting training benchmark...
2025-04-16 21:04:46,213 - INFO - Starting inference benchmark...
2025-04-16 21:04:57,957 - INFO - 
Benchmark Results:
2025-04-16 21:04:57,957 - INFO - Training:
2025-04-16 21:04:57,957 - INFO - Average time per batch: 0.0264 ± 0.0019 seconds
2025-04-16 21:04:57,958 - INFO - Average memory usage: 1133.32 ± 60.67 MB
2025-04-16 21:04:57,958 - INFO - Average throughput: 1217.44 ± 84.68 samples/second
2025-04-16 21:04:57,958 - INFO - Average loss: 6.7575 ± 0.8990
2025-04-16 21:04:57,958 - INFO - Average perplexity: 1429.5823 ± 2116.2029
2025-04-16 21:04:57,958 - INFO - 
Inference:
2025-04-16 21:04:57,959 - INFO - Average time per batch: 0.0102 ± 0.0010 seconds
2025-04-16 21:04:57,959 - INFO - Average memory usage: 879.74 ± 91.12 MB
2025-04-16 21:04:57,959 - INFO - Average throughput: 3170.19 ± 279.44 samples/second
2025-04-16 21:04:57,959 - INFO - Average perplexity: 3164.9513 ± 6489.3211
